#LyX 2.2 created this file. For more info see http://www.lyx.org/
\lyxformat 508
\begin_document
\begin_header
\save_transient_properties true
\origin unavailable
\textclass article
\use_default_options true
\maintain_unincluded_children false
\language english
\language_package default
\inputencoding auto
\fontencoding global
\font_roman "default" "default"
\font_sans "default" "default"
\font_typewriter "default" "default"
\font_math "auto" "auto"
\font_default_family default
\use_non_tex_fonts false
\font_sc false
\font_osf false
\font_sf_scale 100 100
\font_tt_scale 100 100
\graphics default
\default_output_format default
\output_sync 1
\bibtex_command default
\index_command default
\paperfontsize default
\spacing single
\use_hyperref false
\papersize default
\use_geometry true
\use_package amsmath 1
\use_package amssymb 1
\use_package cancel 1
\use_package esint 1
\use_package mathdots 1
\use_package mathtools 1
\use_package mhchem 1
\use_package stackrel 1
\use_package stmaryrd 1
\use_package undertilde 1
\cite_engine basic
\cite_engine_type default
\biblio_style plain
\use_bibtopic false
\use_indices false
\paperorientation portrait
\suppress_date false
\justification true
\use_refstyle 1
\index Index
\shortcut idx
\color #008000
\end_index
\leftmargin 2cm
\topmargin 2cm
\rightmargin 2cm
\bottommargin 2cm
\headheight 0cm
\headsep 0cm
\footskip 0cm
\secnumdepth 3
\tocdepth 3
\paragraph_separation indent
\paragraph_indentation 0bp
\quotes_language english
\papercolumns 1
\papersides 1
\paperpagestyle default
\tracking_changes false
\output_changes false
\html_math_output 0
\html_css_as_file 0
\html_be_strict false
\end_header

\begin_body

\begin_layout Title
High-Order and PWM Based Hidden Markov Model (HOP-HMM)
\end_layout

\begin_layout Section*
Abstract
\end_layout

\begin_layout Section*
Introduction
\end_layout

\begin_layout Standard
[enhancer background]
\end_layout

\begin_layout Standard
[PWMs and motif to classify tissue specific enhancers]
\end_layout

\begin_layout Standard
[k-mer to classify tissue specific enhancers]
\end_layout

\begin_layout Standard
[HMM to classify tissue specific enhancers]
\end_layout

\begin_layout Standard
[other machine learning work to classify tissue specific enhancers]
\end_layout

\begin_layout Standard
[Why the HOP-HMM approach to the problem differently]
\end_layout

\begin_layout Section*
Background
\end_layout

\begin_layout Standard
[generative models]
\end_layout

\begin_layout Standard
[sequences that could have been generated by HOP-HMM]
\begin_inset Newline newline
\end_inset


\begin_inset Newline newline
\end_inset


\end_layout

\begin_layout Section*
Methods
\end_layout

\begin_layout Standard
Generalized Hidden Markov Model (GHMM)
\end_layout

\begin_layout Standard
HOP-HMM is a type of Generalized Hidden Markov Model (GHMM), which is a
 generative model, meaning it relies on the assumption that the observed
 DNA sequence was generated by a parameterized model 
\begin_inset Formula $\theta$
\end_inset

.
 Our goal is to find a model that fits the set of training sequences or
 in other words a 
\begin_inset Formula $\theta$
\end_inset

 that maximizes the log likelihood of the observed sequence.
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{align*}
l(x_{1},...,x_{L}|\theta) & =log\ P(x_{1},...,x_{L}|\theta)\\
\end{align*}

\end_inset

 
\end_layout

\begin_layout Standard
As in a standard HMM, we assume an underlying hidden variables are present,
 and these variables indicate the state of the model while creating the
 sequence.
 As seen in figure 2 and figure 3, 
\end_layout

\begin_layout Standard
TOMMY:
\end_layout

\begin_layout Standard
Generative model.
 A set of sequences could have been generated by this model.
 Our goal is to optimize the model parameters such that the likelihood is
 optimized.
 While so, we would "annotate" the sequences (explain how and why).
 Begin by describing the model automaton (state machine) with a figure.
 Only then talk about the emissions.
 Wrap it up by describing how data is generated (according to the model)
 and how this would help you understand enhancer sequences.
\end_layout

\begin_layout Standard

\series bold
\bar under
Setup
\end_layout

\begin_layout Standard
Let us consider a high-order emission base-states and PWM emission sub-states
 HMM from a dataset of N observations sequences 
\begin_inset Formula $X=\left(X_{1},...,X_{N}\right)$
\end_inset

 where each observation sequence is L nucleotides long 
\begin_inset Formula $X_{i}=\left(x_{1}^{i},...,x_{L}^{i}\right)$
\end_inset

.
 We assume an underlying hidden variable sequences 
\begin_inset Formula $Y=\left(Y_{1},...,Y_{N}\right)$
\end_inset

 where each underlying sequence is also L variables long 
\begin_inset Formula $Y_{i}=y_{1}^{i},...,y_{L}^{i}$
\end_inset

.
 Let the space of underlying states be 
\begin_inset Formula $\Upsilon=\left\{ 1,2,...,m\right\} \times\left\{ 0,1,...,k\right\} $
\end_inset

 
\begin_inset Newline newline
\end_inset


\end_layout

\begin_layout Standard
TOMMY:
\end_layout

\begin_layout Standard
quite abstract and hard to follow.
 try to be more concrete.
 See above.
 Add Figure!
\end_layout

\begin_layout Standard
not clear enough.
 I think your best option is to separate the figure into two parts, like
 I said before.
 Begin by a single "layer" and first describe the automaton and the transition
 probabilities (maybe with a matrix).
 Then show the generated sequence with the (hidden) states above.
 You can also plot the dependencies among them (Markovian model).
 Then make the model more complex, and keeping explaining with automaton
 figures.
\end_layout

\begin_layout Standard

\series bold
\bar under
Emission and Transition
\end_layout

\begin_layout Standard
Underlying states emit the observed sequence are of two types: base-states
 and their sub-states.
 We mark the j'th base-state as 
\begin_inset Formula $(j,0)$
\end_inset

 for 
\begin_inset Formula $j\in\{1,...,m\}$
\end_inset

 and its l'th sub-state as 
\begin_inset Formula $(j,l)$
\end_inset

 for 
\begin_inset Formula $l\in\{1,...,k\}$
\end_inset

.
 Denote the base-state emission order by o, meaning a base-state emits a
 letter sampled from an emission matrix 
\series bold

\begin_inset Formula $E$
\end_inset

 
\series default
that depends on previous 
\begin_inset Formula $o-1$
\end_inset

 letters.
\end_layout

\begin_layout Standard
Sub-state emits multiple letters sampled from a PWM that is fixed and isn't
 learned in the training.
 Denote
\begin_inset Formula $W_{l}$
\end_inset

 the PWM of the l'th sub-states, which is shared between the l'th sub-states
 of all base-states.
 
\begin_inset Newline newline
\end_inset


\begin_inset Newline newline
\end_inset


\begin_inset Graphics
	filename Figures/Slide4.jpg
	scale 40

\end_inset


\begin_inset Newline newline
\end_inset


\series bold
\shape italic
Figure 4: emission and transition process between base-state.
 The upper flow represent transition from base-state to the same base-state,
 through a sub-state that emits a motif.
 The lower flow represent transition between two base-state using the T
 transition matrix, similar to the conventional HMM.
\begin_inset Newline newline
\end_inset


\series default
\shape default

\begin_inset Newline newline
\end_inset


\end_layout

\begin_layout Standard
After emitting a single letter, the j'th base-state has a probability 
\begin_inset Formula $F_{j}$
\end_inset

 to make a transition into one of its sub-state and emit a motif and probability
 
\begin_inset Formula $1-F_{j}$
\end_inset

 to make a transition into one of the base-states and emit a single letter.
 The distribution of transitions between base-states is set by T matrix,
 and between base-state to its sub-states by G matrix.
 After emitting a motif in a sub-state, the next state will be the sub-state's
 base-state where it will emit a single letter.
\begin_inset Newline newline
\end_inset


\end_layout

\begin_layout Standard
\begin_inset Graphics
	filename Figures/Slide1.jpg
	scale 50
	clip

\end_inset


\end_layout

\begin_layout Standard

\series bold
\shape italic
Figure 1: The hidden variable states graph of the HOP-HMM.
 The left hexagons represent base-states, and the circles in the right part
 of each row's represent its sub-states.
 
\begin_inset Newline newline
\end_inset


\end_layout

\begin_layout Standard
\begin_inset Graphics
	filename Figures/Slide2.JPG
	scale 50
	clip

\end_inset


\end_layout

\begin_layout Standard

\series bold
\shape italic
Figure 2: high-order emission of base-states.
 Each emission is dependent on the hidden base-state and o-1 previous observatio
ns.
\begin_inset Newline newline
\end_inset


\end_layout

\begin_layout Standard
\begin_inset Graphics
	filename Figures/Slide3.JPG
	scale 50

\end_inset


\end_layout

\begin_layout Standard

\series bold
\shape italic
Figure 3: PWM emission of sub-states.
\series default
\shape default

\begin_inset Newline newline
\end_inset


\begin_inset Newline newline
\end_inset


\end_layout

\begin_layout Standard

\series bold
\bar under
Parameters
\end_layout

\begin_layout Standard
An HOP-HMM 
\begin_inset Formula $\theta=\{\pi,E,T,G,F\}$
\end_inset

 is parameterized by: 
\end_layout

\begin_layout Itemize
\begin_inset Formula $\pi:\text{ }m\times1$
\end_inset

 initial base-state distribution vector
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
\pi_{j}=P(y_{1}^{i}=j)
\]

\end_inset


\end_layout

\begin_layout Itemize
\begin_inset Formula $E:\text{ }m\times\underset{o\ times}{\underbrace{4\times4\times...\times4}}$
\end_inset

 the base-state high-order emission probability matrix
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
E_{j,b_{1},b_{2},...,b_{o}}=P\left(x_{t}^{i}=b_{o}|y_{t}^{i}=(j,0),x_{t-o+1}^{i}=b_{1},...,x_{t-1}^{i}=b_{o-1}\right)
\]

\end_inset


\end_layout

\begin_layout Itemize
\begin_inset Formula $T:\text{ }m\times m$
\end_inset

 the transition probability matrix
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
T_{j_{1},j_{2}}=P\left(y_{t}^{i}=(j_{2},0)|y_{t-1}^{i}=(j_{1},0)\right)
\]

\end_inset


\end_layout

\begin_layout Itemize
\begin_inset Formula $G:\text{ }m\times k$
\end_inset

 the sub-state entry probability matrix
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
G_{j,l}=P\left(y_{t}^{i}=(j,l)|y_{t-1}^{i}=(j,0),l>0\right)
\]

\end_inset


\end_layout

\begin_layout Itemize
\begin_inset Formula $F:\text{ }m\times1$
\end_inset

 the probability to enter one of the sub-states from a base state
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
F_{j}=P\left(l>0|y_{t-1}^{i}=(j,0),y_{t}^{i}=(j,l)\right)
\]

\end_inset

 
\end_layout

\begin_layout Standard
note: since these describe distributions: 
\begin_inset Formula $\underset{b\in\{1,...,n\}}{\sum}E_{i,b_{1},b_{2},...,b_{o-1},b}=1$
\end_inset

 , 
\begin_inset Formula $\underset{l\in\{1,...,k\}}{\sum}G_{j,l}=1$
\end_inset

 and 
\begin_inset Formula $\underset{j_{2}\in\{1,...,m\}}{\sum}T_{j_{1},j_{2}}=1$
\end_inset


\end_layout

\begin_layout Standard
note 2: we marked here the index of the sequence by 
\begin_inset Formula $i\in\{1,...,N\}$
\end_inset


\end_layout

\begin_layout Subsection*
EM Algorithm
\end_layout

\begin_layout Standard
Our task 
\end_layout

\begin_layout Subsubsection*
E-Step
\end_layout

\begin_layout Standard

\series bold
\bar under
Forward Algorithm
\end_layout

\begin_layout Standard
denote 
\begin_inset Formula $L_{M}(\overline{x})$
\end_inset

 as the likelihood of motif 
\begin_inset Formula $\overline{x}$
\end_inset

 , i.e.
 the probability that 
\begin_inset Formula $\overline{x}$
\end_inset

 was generate by PWM M
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
L_{M}(\overline{x})=P(\overline{x}|M)=\underset{i\in\{1,...,|\overline{x}|\}}{\prod}M_{\overline{x}_{i},i,}
\]

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Formula $\alpha-N\times m\times L$
\end_inset


\end_layout

\begin_layout Standard
we generate 
\begin_inset Formula $\alpha$
\end_inset

 by iterating over 
\begin_inset Formula $t=1,2,...,L$
\end_inset

 for 
\begin_inset Formula $t=0$
\end_inset

: 
\end_layout

\begin_layout Standard
\begin_inset Formula $\alpha_{i,j,0}=P(y_{0}^{i}=j)=\pi_{j}$
\end_inset

 
\end_layout

\begin_layout Standard
For 
\begin_inset Formula $t\in\{1...L\}$
\end_inset

 the table is filled dynamically:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{align*}
\alpha_{i,j,t}= & P(y_{t}^{i}=j,x_{1:t}^{i})=\\
= & \underset{\text{base-state step}}{\underbrace{\sum_{j'\in\{1,...,m\}}\alpha_{i,j',t-1}\cdot\left(1-F_{j'}\right)\cdot T_{j',j}\cdot E_{j,x_{t-o+1}^{i},...,x_{t}^{i}}}}\\
+ & \underset{\text{sub-state step}}{\underbrace{\sum_{l\in\{1,...,k\}}\alpha_{i,j,t-|W_{l}|-1}\cdot F_{j}\cdot G_{j,l}\cdot L_{W_{l}}\left(x_{t-|W_{l}|}^{i},...,x_{t-1}^{i}\right)\cdot E_{j,x_{t-o+1}^{i},...,x_{t}^{i}}}}
\end{align*}

\end_inset


\end_layout

\begin_layout Standard
TODO: how out of range is handled for the PWMs and high-order emission.
\begin_inset Newline newline
\end_inset


\end_layout

\begin_layout Standard

\series bold
\bar under
Backward Algorithm
\end_layout

\begin_layout Standard
\begin_inset Formula $\beta-N\times m\times L$
\end_inset


\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{align*}
\beta_{i,j,t}= & P(y_{t}^{i}=j,x_{t+1:L}^{i})=\\
= & \underset{\text{base-state step}}{\underbrace{\sum_{u\in\{1,...,m\}}\left(1-F_{j}\right)\cdot T_{j,u}\cdot E_{u,x_{t-o+2}^{i},...,x_{t+1}^{i}}\cdot\beta_{i,u,t+1}}}\\
+ & \underset{\text{sub-state step}}{\underbrace{\sum_{l\in\{1,...,k\}}F_{j}\cdot G_{j,l}\cdot L_{W_{l,}}\left(x_{t+1}^{i},...,x_{t+|W_{l}|}^{i}\right)\cdot E_{j,x_{t-o+|W_{l}|+2}^{i},...,x_{t+|W_{l}|+1}^{i}}\cdot\beta_{i,j,t+|W_{l}|+1}}}
\end{align*}

\end_inset

 
\end_layout

\begin_layout Standard
TODO: how out of range is handled for the PWMs.
 The problem of peaking before the t when doing a high-order emission of
 the base-states
\begin_inset Newline newline
\end_inset


\end_layout

\begin_layout Standard

\series bold
\bar under
M-Step
\end_layout

\begin_layout Standard
First we calculate auxiliary variables:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{align*}
\psi_{i,j,l,t}= & P\left(y_{t}^{i}=(j,0),y_{t+1:t+|W_{l}|}^{i}=(j,l),y_{t+|W_{l}|+1}^{i}=(j,0),X_{i}\right)\\
= & \alpha_{i,j,t}\cdot F_{j}\cdot G_{j,l}\cdot L_{W_{l,}}\left(x_{t+1}^{i},...,x_{t+|W_{l}|}^{i}\right)\cdot E_{j,x_{t+|W_{l}|-o+2}^{i},...,x_{t+|W_{l}|+1}^{i}}\cdot\beta_{i,j,t+|W_{l}|+1}
\end{align*}

\end_inset


\begin_inset Newline newline
\end_inset


\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{align*}
\gamma_{i,j,t}= & P\left(y_{t}^{i}=(j,0)|X_{i}\right)=\frac{P\left(y_{t}^{i}=(j,0),X_{i}\right)}{P\left(X_{i}\right)}\\
= & \frac{\alpha_{i,j,t}\cdot\beta_{i,j,t}}{\underset{j'\in\{1,...,m\}}{\sum}\left(\alpha_{i,j',t}\cdot\beta_{i,j',t}+\underset{l\in\{1,...,k\}}{\sum}\underset{s\in\{1,...,|W_{l}|\}}{\sum}\psi_{i,j',l,t-s}\right)}\\
\end{align*}

\end_inset


\begin_inset Newline newline
\end_inset


\end_layout

\begin_layout Standard
TODO: does different t gives different 
\begin_inset Formula $P\left(X_{i}\right)=\underset{j'\in\{1,...,m\}}{\sum}\alpha_{i,j',t}\cdot\beta_{i,j',t}$
\end_inset

? Should it?
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{align*}
\xi_{i,j_{1},j_{2},t}= & P\left(y_{t}^{i}=(j_{1},0),y_{t+1}^{i}=(j_{2},0)|X_{i}\right)=\frac{P\left(y_{t}^{i}=(j_{1},0),y_{t+1}^{i}=(j_{2},0),X\right)}{P\left(X_{i}\right)}\\
\\
= & \frac{\alpha_{i,j_{1},t}\cdot\left(1-F_{j_{1}}\right)\cdot T_{j_{1},j_{2}}\cdot E_{j_{2},x_{t-o+2}^{i},...,x_{t+1}^{i}}\cdot\beta_{i,j_{2},t+1}}{\underset{j'_{1},j'_{2}\in\{1,...,N\}}{\sum}\alpha_{i,j'_{1},t}\cdot\left(1-F_{j'_{1}}\right)\cdot T_{j'_{1},j'_{2}}\cdot E_{j'_{2},x_{t-o+2}^{i},...,x_{t+1}^{i}}\cdot\beta_{i,j'_{2},t+1}+\underset{j',\in\{1,...,N\}\,l\in\{1,...,k\}}{\sum}\left(\underset{s\in\{0,...,|W_{l}|\}}{\sum}\psi_{i,j',l,t-s}\right)}
\end{align*}

\end_inset


\begin_inset Newline newline
\end_inset

TODO: maybe denote a new variable to make above formula nicer?
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{align*}
\eta_{i,j,l,t}= & P\left(y_{t}^{i}=(j,0),y_{t+1:t+|W_{l}|}^{i}=(j,l)|X_{i}\right)\\
= & \frac{P\left(y_{t}^{i}=(j,0),y_{t+1:t+|W_{l}|}^{i}=(j,l),X_{i}\right)}{P\left(X_{i}\right)}\\
= & \frac{\psi_{i,j,l,t}}{\underset{j'\in\{1,...,m\}}{\sum}\left(\alpha_{i,j',t}\cdot\beta_{i,j',t}+\underset{l\in\{1,...,k\}}{\sum}\underset{s\in\{1,...,|W_{l}|\}}{\sum}\psi_{i,j',l,t-s}\right)}
\end{align*}

\end_inset


\end_layout

\begin_layout Standard

\end_layout

\begin_layout Standard
We use the temporary auxiliary variables to calculate the 
\begin_inset Formula $\theta_{max}$
\end_inset

 that maximizes likelihood of the observations.
\end_layout

\begin_layout Standard
TODO: add mid steps to the calculation to make more readable
\end_layout

\begin_layout Standard
\begin_inset Formula $E_{j,b_{1},b_{2},...,b_{o}}=\frac{\underset{i\in[N]\,t\in[L]}{\sum}\gamma_{i,j,t}\cdot\boldsymbol{1}_{b_{1},...,b_{o}}(x_{t-o+1}^{i},...,x_{t}^{i})}{\underset{i\in[N]\,t\in[L]}{\sum}\gamma_{i,j,t}}$
\end_inset


\end_layout

\begin_layout Standard
\begin_inset Formula $T_{j_{1},j_{2}}=\frac{\underset{i\in[N]\,t\in[L]}{\sum}\xi_{i,j_{1},j_{2},t}}{\underset{i\in[N]\,t\in[L]}{\sum}\gamma_{i,j_{1},t}}$
\end_inset


\end_layout

\begin_layout Standard
\begin_inset Formula $F_{j}=\frac{\underset{i\in[N]\,t\in[L]\,l\in[k]}{\sum}\eta_{i,j,l,t}}{\underset{i\in[N]\,t\in[L]}{\sum}\gamma_{i,j,t}}$
\end_inset


\end_layout

\begin_layout Standard
\begin_inset Formula $G_{j,l}=\frac{\underset{i\in[N]\,t\in[L]}{\sum}\eta_{i,j,l,t}}{\underset{i\in[N]\,t\in[L],\,l'\in[k]}{\sum}\eta_{i,j,l',t}}$
\end_inset


\end_layout

\begin_layout Standard
\begin_inset Formula $\pi_{j}=\frac{\gamma_{i,j,1}}{\underset{i\in[N]\,j'\in[m]}{\sum}\gamma_{i,j',1}}$
\end_inset


\begin_inset Newline newline
\end_inset


\begin_inset Newline newline
\end_inset


\begin_inset Newline newline
\end_inset


\begin_inset Newline newline
\end_inset

[Roadmap enhancers preprocessing]
\end_layout

\begin_layout Standard
[training on roadmap data]
\end_layout

\begin_layout Standard
[classification of regulation modules]
\end_layout

\begin_layout Section*
Results
\end_layout

\begin_layout Standard
[test accuracy on roadmap enhancers]
\end_layout

\begin_layout Standard
[prediction on roadmap regulation modules]
\end_layout

\begin_layout Standard
[Whole genome classification?]
\end_layout

\begin_layout Standard
[Was HOP-HMM better?]
\begin_inset Newline newline
\end_inset


\end_layout

\begin_layout Standard

\series bold
\bar under
Possible Applications
\end_layout

\begin_layout Standard
labeled enhancer seqs from multiple motifs-> EM to learn E M F per floor
 + setting 
\begin_inset Formula $T=\mathbb{I}_{m\times m}$
\end_inset

 -> posterior of whole genome with sliding window -> classify whole genome
\end_layout

\begin_layout Standard
learn E M F -> check correlation with TF expression
\end_layout

\begin_layout Standard
run EM on whole genome -> posterior of whole genome -> check correlation
 of posterior to ChIP-Seq of histone modifications
\end_layout

\begin_layout Standard
E M F T-> posterior of whole genome -> see if known critical SNPs are critical
 in classification
\end_layout

\begin_layout Section*
Discussion
\end_layout

\end_body
\end_document
