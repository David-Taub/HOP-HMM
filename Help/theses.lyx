#LyX 2.2 created this file. For more info see http://www.lyx.org/
\lyxformat 508
\begin_document
\begin_header
\save_transient_properties true
\origin unavailable
\textclass article
\begin_preamble
\usepackage{babel}
\end_preamble
\use_default_options false
\maintain_unincluded_children false
\language english
\language_package default
\inputencoding auto
\fontencoding T1
\font_roman "default" "default"
\font_sans "default" "default"
\font_typewriter "default" "default"
\font_math "auto" "auto"
\font_default_family default
\use_non_tex_fonts false
\font_sc false
\font_osf false
\font_sf_scale 100 100
\font_tt_scale 100 100
\graphics default
\default_output_format default
\output_sync 1
\output_sync_macro "\synctex=1"
\bibtex_command default
\index_command default
\paperfontsize default
\spacing single
\use_hyperref false
\papersize default
\use_geometry true
\use_package amsmath 2
\use_package amssymb 2
\use_package cancel 0
\use_package esint 1
\use_package mathdots 0
\use_package mathtools 0
\use_package mhchem 0
\use_package stackrel 0
\use_package stmaryrd 0
\use_package undertilde 0
\cite_engine natbib
\cite_engine_type numerical
\biblio_style plainnat
\use_bibtopic false
\use_indices false
\paperorientation portrait
\suppress_date false
\justification true
\use_refstyle 0
\index Index
\shortcut idx
\color #008000
\end_index
\leftmargin 2cm
\topmargin 2cm
\rightmargin 2cm
\bottommargin 2cm
\headheight 0cm
\headsep 0cm
\footskip 0cm
\secnumdepth 3
\tocdepth 3
\paragraph_separation skip
\defskip medskip
\quotes_language english
\papercolumns 1
\papersides 1
\paperpagestyle default
\tracking_changes false
\output_changes false
\html_math_output 0
\html_css_as_file 0
\html_be_strict false
\end_header

\begin_body

\begin_layout Title
High-Order Generalized Hidden Markov Model for DNA Regulatory Sequences
 Classification
\end_layout

\begin_layout Section*
Abstract
\end_layout

\begin_layout Subparagraph*
[TODO: rewrite this]
\end_layout

\begin_layout Standard
The TF inside the nucleus of specific tissues are thought to be a key factor
 in the activation of specific enhancer.
 The TFs form a transcription complex and are connected to the enhancer
 and promoter sequences on top of the TF binding sites (TFBS).
 Studies using TFBS of TF present in specific cell types are used to classify
 cell specific enhancer sequences.
 show heat map of AUC-ROC results.
 Between these TFBS, k-mer frequencies varies between enhancers and non
 regulatory 
\begin_inset Quotes eld
\end_inset

backgound
\begin_inset Quotes erd
\end_inset

 DNA, and was used to classify enhancers from background using only the
 k-mer distribution (Inbar and tomy, gkm-SVM), and is thought to play a
 role in spacial properties, nucleosome location and cleavage that cause
 accessibility of near-by TFBS.
 Using 44 out of 127 epigenetic data of Roadmap Project to select tissue
 specific enhancer sequences dataset.
 In our method, we look for different TFBS and k-mer presence in sequences
 to classify cell-specific sequences, inside regulatory modules.
\end_layout

\begin_layout Section*
Background
\end_layout

\begin_layout Subsection*

\series bold
\bar under
The Genome
\end_layout

\begin_layout Standard
The genome of every organism contains the inherited information that defines
 its complex structure and function.
 The genome is built out of Deoxyribonucleic acid (DNA) molecule, that is
 a built out of two chains of nucleotides units that form a double helix
 shape.
 Each nucleotide is built our of 4 different types bases: cytosine, guanine,
 adenine or thymine or in short A,C,G and T.
 The nucleotides are organized in pairs called base pairs where each of
 the paired nucleotides are complimentary to each other and provide redundancy.
 
\end_layout

\begin_layout Standard
Proteins are macromolecoles, which carry various roles and functions within
 organisms.
 They are built out of 20 different amino acids, which order and structure
 is encoded inside genetic segments in the genome called genes.
 Through the transcription and translation processes, the genes are expressed
 and result in the formation of proteins.
 In the transcription process the gene is read and transcribed into a single
 strand sequence of RNA.
 Later, the RNA molecules are translated into a sequence of amino acids
 that constitute a protein.
\end_layout

\begin_layout Paragraph*

\series bold
\bar under
Genes
\end_layout

\begin_layout Standard
Gene sequences are built out fragmented introns and exons, where only the
 exons becomes the RNA molecules that translates into proteins while the
 introns are spliced away beforehand.
 Although the exons alone hold the recipe for the construction of the organism's
 proteins, the complexity of the organism is not a product of their number
 or their length.
 For example, the humans and Caenorhabditis elegans roundworms both have
 about 19,000 genes (Ezkurdia et al.
 2014; The C.
 elegans Sequencing Consortium 1998), with roughly the same total exon length
 and number, although the human body is vastly more diverse and complex.
 The source for the organisms complexity differences is attributed to the
 gene regulation mechanism.
 The human genome is 3.23 Gb long, and it is estimated that gene regulation
 involves 10-20% of it (Pennacchio et al.
 2015), compared to 1% that are exon regions (Ng et al.
 2009).
 
\end_layout

\begin_layout Paragraph*

\series bold
\bar under
Enhancers
\end_layout

\begin_layout Standard
Enhancers are are non-coding regulatory DNA sequences that play a key role
 in the regulation transcription of genes.
 In humans there are hundreds of thousands of enhancers, scattered over
 the non-coding regions of the genome, and their length are usually between
 100-1000 bp.
 When activated, the DNA folding draws the enhancer spatially closer to
 another type of regulatory element called promoter, resulting in the translatio
n of a gene adjacent to the promoter (see figure 1).
 The enhancer's target gene is the expressed gene from this activation process.
 It can be located up to a megabase upstream or downstream from their activating
 enhancer (May et al.
 2011), and are orientation independent to it.
 Moreover, the gene-enhancer connection is not exclusive, and the common
 case is that each enhancer has several target genes and vice versa (Fishilevich
 et al.
 2017).
\end_layout

\begin_layout Standard
\begin_inset Graphics
	filename Figures/Enhancer_gene_transcription.png
	scale 50

\end_inset


\end_layout

\begin_layout Standard

\series bold
\bar under
Figure:
\series default
\bar default
 1) An enhancer and its distal target gene.
 2) The DNA folds and the attached with transcription factors draw other
 co-factor proteins that together form the transcription complex.
 3) The RNA Polymerase II is recruited and while moving along the gene it
 generates a new RNA molecule that is transcribed off the gene sequence.
\end_layout

\begin_layout Paragraph*

\series bold
\bar under
Transcription Factor Binding
\end_layout

\begin_layout Standard
Transcription factors (TF) are proteins that bind to the DNA, and together
 with other co-factor proteins initiate the gene transcription process.
 TFs tend to bind to certain transcription factor binding sites (TFBS),
 which are motifs of nucleotides on the DNA with average length of 12 bp
 in humans (Kulakovskiy et al.
 2011) that are conserved between species (Doniger et al.
 2005).
 On genome-wide association studies (GWAS) done with ChIP-seq method, different
 TFs have different distributions of TFBS they are observed attached to
 (Khan et al.
 JASPAR 2018; Gheorghe et al.
 2018).
\end_layout

\begin_layout Standard
Both enhancers and promoters contain TFBSs that are critical for the their
 correct regulatory operation.
 Multiple studies have shown that genetic alternations in TFBS can affect
 the expression of the regulated gene and are a major cause of different
 human diseases (Miguel-Escalada 2015; Soldner 2016; Smemo S.
 2012; Benko 2009; Emison, 2005; Lettice 2003).
 From the sequence aspect, enhancers and promoters have a similar structure
 of a background nucleotide sequence with distribution different from other
 part of the genome, with TFBS motifs tiled inside this background sequence.
 
\end_layout

\begin_layout Standard
The enrichment of TFBS is a good predictor for the location of promoter
 and enhancer regulatory regions and the type of cells they will be active
 in.
 Folding of DNA allows the enhancer-promoter interactions, in which the
 TFs take major part.
 Once bounded to the DNA, the TFs recruit other cofactor proteins to them,
 and together they form a transcription preinitiation complex (PIC), a very
 large assembly of proteins.
 Out of the tens of proteins constructing the PIC, the sub-unit RNA Polymerase
 (RNA pol II) has the role of transcribing the adjacent gene.
 it opens the double stranded DNA, so that one strand of nucleotides is
 exposed and becomes a template for RNA synthesis.
\end_layout

\begin_layout Paragraph*

\series bold
\bar under
PWMs
\end_layout

\begin_layout Standard
Generating a compact model for estimating the binding potential of a DNA
 sequence to a TF, i.e.
 
\begin_inset Formula $P(x_{1:n}|binding)$
\end_inset

, is not trivial as might seem on first look.
 The peaks of the ChIP-seq data are used as the ground truth of TF binding
 locations, from which the model is built.
 Position weight matrix (PWM) is a commonly used simplistic method to address
 this task.
 The underlying assumption of the PWM model is that every position in the
 DNA sequence has an independent probability to attach to the TF, and therefore
 the total binding probability is a multiplication of all the per-position
 probabilities in the motif:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
P(x_{1:n}|binding)=\prod_{i\in[n]}P(x_{i}|binding)
\]

\end_inset

Where n is the size of relevant sequence.
 The size of the sequence that is affected by the binding event is derived
 from the physical characteristics of the TF.
\end_layout

\begin_layout Standard
\begin_inset Formula $P(x_{i}|binding)$
\end_inset

 is estimated by counting the nucleotides frequency in every position of
 the observed binding sites, which are the ChIP-seq peaks.
 For a motif of length J, this probability estimation is stored in a PWM
 matrix W as followed: 
\begin_inset Formula $W_{i,j}=\frac{1}{N}\sum_{k\in N}\boldsymbol{1}(X_{i,k}=j)$
\end_inset

 where 
\begin_inset Formula $i\in[J]$
\end_inset

 the position in the motif and 
\begin_inset Formula $j\in[4]$
\end_inset

 the nucleotide index of A,C,G and T.
 
\end_layout

\begin_layout Standard
From a generative model point of view, the sequence is generated by a TFBS
 motifs emission system.
 For this needs, the log of the matrix often comes handy for calculation
 of 
\begin_inset Formula $log\left(L(W;x_{1:n})\right)$
\end_inset

, the log of the probability that a motif was generated by a PWM W.
 This calculation is done be a convolution of 
\begin_inset Formula $log(W)$
\end_inset

 on a one-hot encoding of the sequence.
\end_layout

\begin_layout Standard
\begin_inset Graphics
	filename Figures/pwm_mult.png
	scale 50

\end_inset


\end_layout

\begin_layout Standard

\series bold
Figure 3:
\series default
 a sub-sequence out of the DNA is represented in a one-hot encoding, then
 entry-wise multiplied with the PWM.
 Then, the sum of the logs of the maximal values of each column in the result
 matrix is calculated, which is the log likelihood of the TF binding to
 the sub-sequence.
 This log likelihood is calculated for each location in the sequence, where
 location with high values indicate high likelihood of TF binding.
\end_layout

\begin_layout Paragraph*

\bar under
Inter TFBS Sequences and Conservation
\end_layout

\begin_layout Standard
Conserved non-coding elements (CNE) reside in clusters, usually with low
 gene density but with vicinity to genes.
 Typically, CNE are structured in arrays called GRB, with a mean length
 of 1.4 Mb (Dong et al.
 2009).
 The correlation between conservation of non-coding region and enhancer
 functionality is not strong.
 Some verified enhancers are weakly or not conserved between distant species
 (Friedli et al.
 2010; Rosin et al.
 2013; Taher 2011; Lindblad-Toh 2011) and some highly conserved areas in
 the mouse genome are not associated to regulatory activity and their deletion
 and yielded viable mice (Ahituv et al.
 2007).
 Nevertheless, an assay of elements with 100% sequence identity of over
 200 bp between human and mouse found that 50% showed enhancers activity
 in mice (Visel et al., 2007).
 The reason for such ultra-conservation of 200 bp sequences when the TFBS
 is only 4-8 bp long is unclear.
 It is possible that these conserved sequences are actually long assembly
 of overlapping TFBS or that the enhancer has another function as a eRNA,
 that the exact nature of its mechanisms is no understood (Haeussler et
 al.
 2011, Andersson et al.
 2014).
\end_layout

\begin_layout Subsection*

\series bold
\bar under
Epigenetics
\end_layout

\begin_layout Standard
Almost all cells in every organism contain its genome, but only part of
 genome is active in any specific cell.
 Cells of different types and in different operation modes differ by gene
 expression patterns.
 The reason for that lies in regulation components that are outside of the
 genomic sequence.
 The location and presence of TFBS, background nucleotides distribution
 and other sequence related properties are not enough to explain regulatory
 role of regions in the genome.
 
\end_layout

\begin_layout Standard
Several epigenetic features (which do not involve the nucleotides sequence
 directly) correlate with enhancer regions in the genome and imply their
 target-gene:
\end_layout

\begin_layout Itemize
Accessibility
\end_layout

\begin_layout Itemize
TF & cofactors binding
\end_layout

\begin_layout Itemize
Histone modifications
\end_layout

\begin_layout Itemize
Topologically associating domain (TAD) patterns
\end_layout

\begin_layout Itemize
DNA methylation
\end_layout

\begin_layout Standard
These properties and mechanisms have measurable features that lie on top
 of the genome.
 Their combination is the main source of identification for enhancer regions
 in the genome.
 Each cell has its own epigenetic features, in a binaric form, e.g.
 a specific part of the genome can be either accessible, or not.
 When several similar cells from the same tissue sample are measured, a
 frequency or count of the feature is measured per DNA loci, and generates
 epigenetic data.
 The epigenetic data is commonly used as the ground truth indication for
 enhancer sequences, as done for the human genome in the ENCODE project.
\end_layout

\begin_layout Subparagraph*
[TODO: fix ENCODE cite above]
\end_layout

\begin_layout Subparagraph*
[Tommy: add figure of epigenetics tracks, genome browser tracks on top of
 the DNA]
\end_layout

\begin_layout Paragraph

\series bold
\bar under
Accessibility
\end_layout

\begin_layout Standard
In eukaryotes, the DNA is packed around a structure of 8 histone proteins,
 together forming a nucleosome core.
 The location of the nucleosome binding is not random over the DNA sequence,
 but has a tendency for specific DNA binding sites (Cutter et al.
 2015).
 DNA that is wrapped around a nucleosome has a lesser probability to interact
 with proteins, as it is physically inaccessible.
 Both the enhancer, the promoter and the gene need to be accessible for
 a successful transcription.
\end_layout

\begin_layout Standard
Since the scenario of TF binding on an enhancer requires an accessible DNA
 region, I hypersensative sites are used for detecting a potential DNA cleavages
 that have the potential of being regulatory elements, in usually a better
 resolution than histone marks.
\end_layout

\begin_layout Paragraph

\series bold
\bar under
TADs
\end_layout

\begin_layout Subparagraph*
[TODO: rearrange]
\end_layout

\begin_layout Standard
TADs are a sections of the genome where the DNA is self interact more frequently.
 Enhancer-gene interactions have tendency to happen within the region of
 TADs.
 Hi-C are a set of techniques for measuring the spatial organization of
 chromatin in the nucleus.
 These procedures measure the number of occurrences were genome loci are
 in proximity.
 In recent years, Hi-C data was generated from multiple organisms (Dixon
 et al.
 2012; Rao et al.
 2014; Dixon et al.
 2015; Sexton et al.
 2012), and shown that the genome is partitioned into TADs, mostly under
 10 mb long.
 Within TADs sequence to sequence interaction are more frequent.
 TADs boundaries are enriched with DNA-biding protein motifs, and their
 location play a significant role in gene regulation.
 Gene-enhancer interactions occur when both are in the same TAD, and deletion
 of the motifs from TAD's boundaries may cause changes in the gene activation.
 It has been argued (Kadauke et al.
 2018) that the TADs are dynamic throughout the cell's life cycle, and with
 the change of the TADs structure the enhancer-gene activation couples may
 vary as well, suggesting the target gene is not predetermined by the the
 enhancer's sequence.
\end_layout

\begin_layout Paragraph*

\series bold
\bar under
Histone Marks
\end_layout

\begin_layout Standard
Chromatin modifications signatures, also called histone marks, are predictive
 of enhancer position and activity status and can be assessed (Visel et
 al.
 2009; Firbi et al.
 2010; Fernandez et al.
 2012).
 The histone marks are considered to contain a certain 
\begin_inset Quotes eld
\end_inset

histone code
\begin_inset Quotes erd
\end_inset

 which encode complex information, additionally to the DNA, regarding the
 transcription regulation and other aspects.
 Comparing to other epigenetic information, and especially DNA methylation
 (Przybilla et al.
 2012), chromatin modifications have a short time-scale of seconds or hours
 (Hayashi-Takanaka et al., 2011), hence they are considered part of the dynamic
 changes of the cell's modes.
 
\end_layout

\begin_layout Standard
H3K4me1 and H3K27ac are among the predominant histone marks of active enhancers,
 where H3K4me1 are enriched on transcribed genes and enhancers prior to
 activation (calo et al.
 2013), and is thought to precede the H3K27ac modification (Creyghton et
 al., 2010; Rada-Iglesias et al., 2011; Zentner et al., 2011) which is known
 to occur during the activation.
 Other histone marks that are present on active enhancers and are used for
 their detection are H3K9ac (Ernst et al., 2011; Karmodiya et al., 2012; Krebs
 et al., 2011; Zentner et al., 2011) and H3K18ac (Jin et al., 2011).
 Even though H3K27ac have been identified as an important mark for distinguishin
g active enhancers from poised enhancers (Creyghton et al.
 2010), it is not enough as its own since when present alongside H3K4me3
 it is an indication for active promoters [Heintzman et al., 2007].
 In contrast, H3K27ac absence and H3K4me1and H3K27me3 enrichment are typical
 for poised enhancers (Creyghton et al, 2010).
\begin_inset Newline newline
\end_inset


\end_layout

\begin_layout Standard
\begin_inset Graphics
	filename Figures/Enhancers_status.png
	scale 50

\end_inset


\end_layout

\begin_layout Standard

\series bold
\bar under
Figure:
\series default
\bar default
 The accessibility of the enhancer's sequence and its surrounding histone
 marks are connected to its regulatory activity state.
 On the upper part an active enhancer sequence that is accessible for protein
 interaction needed for transcription, where as on the lower part an inactive
 enhancer is inaccessible since it is wrapped around a nucleosome.
\end_layout

\begin_layout Paragraph*

\series bold
\bar under
DNA Methylation
\end_layout

\begin_layout Standard
Here we covered the main known gene regulation mechanisms, but there are
 more regulation methods which have been researched, and are note negligible.
 DNA methylation at cytosine and CpG sites has been involved in genome silencing
 in multiple processes (Jones et al.
 2012), and has been documented as largely correlated with gene expression
 inhibition when present in promoters.
 In enhancer elements, anticorrelation was found between DNA methylation
 density and enrichment of active enhancer histone marks and TF binding
 (Stadler et al., 2011; Thurman et al., 2012), although the cause and consequence
 relationship underlying these correlations is not yet clear.
 
\end_layout

\begin_layout Paragraph
Epigenetics Limitation
\end_layout

\begin_layout Standard
The currently most accurate method for predicting the location of tissue
 specific enhancers in a genome wide scale, is analyzing the histone marks
 and TF and cofactors presence using ChIP-seq from a cell line or from a
 tissue, combined with DNase I hypersensative (DHS).
 The main disadvantage of this method is this process is inherently limited
 to the tissues we can extract and isolate for the epigenetic examination.
 Another disadvantage of this method is the need for live cells for the
 verification of the regulatory activity of a sequence.
 The persecute for an efficient computational method for predicting the
 functional nature of sequences 
\begin_inset Quotes eld
\end_inset

in-silico
\begin_inset Quotes erd
\end_inset

 has produced positive, yet far from sufficient results in the last years,
 as reviewed in (Kleftogiannis et al.
 2016).
\end_layout

\begin_layout Subsection*
Computational Biology & Machine Learning 
\end_layout

\begin_layout Subparagraph*
#######
\end_layout

\begin_layout Subparagraph*
[tommy: add a chapter about other methods.
 explain more deeply about each of these, add data representation chapter]
\end_layout

\begin_layout Standard
There are several achievements in the task of predicting epigenetic and
 regulatory properties of DNA elements given only their sequence using machine
 learning algorithms.
 DeepSEA (Zhou and Troyanskaya, 2015) deep convolutional network is fed
 with 1000 bp DNA sequence and predicts an output vector of 919 binary features
 which represents the chromatin modifications of 200 bp bin in the center
 of the input sequence.
 The training labels used are the chromatin modification are extracted from
 ENCODE and Roadmap Epigenomics data releases.
\end_layout

\begin_layout Standard
Basset (Kelley et al.
 2016) and deltaSVM predicts DNase I sensitivity.
 gkm-SVM (Beer et al.
 2014) uses gapped kmers as features for an SVM classifier to predict the
 role of DNA sequences.
 
\end_layout

\begin_layout Standard
The disadvantage of these method is their need for a training data of known
 regulatory elements, which are known mainly from GWAS surveys done on 127
 obtained human cell types in the Roadmap and ENCODE projects (Kundaje et
 al.
 2015; Ernst et al.
 2011).
 The number of different cell types in the human body is estimated to be
 higher than 2200 (Hatano et al.
 2011, Diehl et al.
 2016), where then number and location of tissue specific enhancers of the
 rest of the cell types is a mystery.
\end_layout

\begin_layout Paragraph*

\series bold
Data Representation
\end_layout

\begin_layout Standard
The DNA sequence, when read from cells, is usually stored in files, such
 as .fa, as a sequence of letters A,C,G and T.
 For an algorithm to process it, the characters are mapped into integers
 1,2,3 and 4 respectively.
 For many algorithms, such as DeepSEA and our HOP-Baum-Welch, it is more
 suitable to represent the DNA sequence in a one-hot encoding as described
 in figure 3.
 
\end_layout

\begin_layout Standard
A common feature extraction technique is representing a DNA sequence as
 a vector of the in-sequence frequencies of all the possible kmer as used
 in gkm-SVM.
 In this technique, similarly to the bag of words technique in text analysis
 and natural language processing, the order of the kmer locations is sacrificed
 for a more meaning-oriented, structured and fixed-length data encoding.
 
\end_layout

\begin_layout Subparagraph
There are other mechanisms and non classic scenarios that involve enhacners:
 are intergenic enhancers, very long enhancers
\end_layout

\begin_layout Subparagraph*
TODO: k-mer frequencies varies between enhancers and non regulatory
\begin_inset Quotes eld
\end_inset

background
\begin_inset Quotes erd
\end_inset

 DNA, and was used to classify enhancers from backgound using only the k-mer
 distribution (Inbar and Tommy),
\end_layout

\begin_layout Subparagraph
TODO: [Levin, VISTA] experiments shows that insertion of a sequence without
 any epigenetic information will activate an enhancer with a near by blue
 coloring gene.
 This implies that for the newly introduced sequences to operate as an enhancer
 and target gene, all that is needed is its sequence in arbitrary location
 without additional epigenetic information.
\end_layout

\begin_layout Subparagraph
From (C.
\begin_inset Quotes eld
\end_inset

Probing instructions for expression regulation in gene nucleotide compositions
\begin_inset Quotes erd
\end_inset

):
\end_layout

\begin_layout Standard
\begin_inset Quotes eld
\end_inset

Several approaches have tackled this problem by modeling gene expression
 based on epigenetic marks, with the ultimate goal of identifying driving
 regions and associated genomic variations that are clinically relevant
 in particular in precision medicine.
 However, these models rely on experimental data, which are limited to specific
 samples (even often to cell lines) and cannot be generated for all regulators
 and all patients.
 In addition, we show here that, although these approaches are accurate
 in predicting gene expression, inference of TF combinations from this type
 of models is not straightforward.
 Furthermore these methods are not designed to capture regulation instructions
 present at the sequence level, before the binding of regulators or the
 opening of the chromatin.
\begin_inset Quotes erd
\end_inset


\end_layout

\begin_layout Subparagraph*
[PWMs and motif to classify tissue specific enhancers]
\end_layout

\begin_layout Subparagraph*
[k-mer to classify tissue specific enhancers]
\end_layout

\begin_layout Subparagraph*
[HMM to classify tissue specific enhancers]
\end_layout

\begin_layout Subparagraph*
[other machine learning work to classify tissue specific enhancers]
\end_layout

\begin_layout Subparagraph*
[Why the HOP-HMM approach to the problem differently]
\end_layout

\begin_layout Subparagraph*
[other stories]
\end_layout

\begin_layout Subparagraph*
[TOMMY: add research questions after background.
 BW and HOP-BW should come after HMM desc and HOP HMM desc.]
\end_layout

\begin_layout Subparagraph*
[Tommy: start with a 2 states before, for gentle intro.
 add the generative model part]
\end_layout

\begin_layout Subparagraph*
[tommy: insert here a part about the goal of the research: identify enhancers
\end_layout

\begin_layout Section*
Methods
\end_layout

\begin_layout Standard
[generative models, HMM]
\end_layout

\begin_layout Paragraph*
Markov Model
\end_layout

\begin_layout Standard
Markov model (Markov, 1906) is a stochastic model named after Andrey Markov
 a Russian mathematician.
 In a Markov model, at any time the model is at one of m states 
\begin_inset Formula $\left\{ S_{1},...,S_{m}\right\} $
\end_inset

, where the first state is sampled from a distribution 
\begin_inset Formula $\pi_{i}=P\left(y_{1}=S_{i}\right)$
\end_inset

 and the probability of transitions between the states is denoted by 
\begin_inset Formula $T_{i,j}=P\left(y_{t}=S_{i}|y_{t-1}=S_{j}\right)$
\end_inset

.
 The model's travel over the states is called a Markov process, and the
 sequence of states visited in the process is called a Makov chain.
\end_layout

\begin_layout Subparagraph*
[Tommy: 4 is confusing, try 3]
\end_layout

\begin_layout Subparagraph*
[tommy: describe the likelihood of the model, starting from 1st state times
 all the transitions]
\end_layout

\begin_layout Standard
\begin_inset Graphics
	filename Figures/Markov_state.png
	scale 40

\end_inset


\end_layout

\begin_layout Standard

\series bold
\emph on
Figure: Markov model with 4 states, marked as 
\begin_inset Formula $S_{1},S_{2},S_{3},S_{4}$
\end_inset

 with two of all possible transition probabilities written.
 T matrix describes the transition distributions, where 
\begin_inset Formula $T_{i,*}$
\end_inset

 is the distribution of the model's next step when in 
\begin_inset Formula $S_{i}$
\end_inset

.
\end_layout

\begin_layout Subsection*
Hidden Markov model 
\end_layout

\begin_layout Standard
Multiple signal processing algorithms have been used in computational biology,
 and HMM is especially popular among them.
 Hidden Markov model (HMM) is a statistical model proposed by Leonard Baum
 (Baum et al.
 1966) and is based on the Markov model for modeling regions with alternating
 frequencies of patterns and symbols.
 It was used in various engineering fields since the 1980s especially in
 speech recognition, character recognition and digital communication and
 was adopted in the computational biology field.
 
\end_layout

\begin_layout Standard
Hidden Markov model (HMM) is a model that travels over hidden states in
 a Markov process, and while doing so it emits variables called observed
 variables.
 
\end_layout

\begin_layout Standard
As a generative model, HMM relies on the assumption that the observed DNA
 sequence of length L 
\begin_inset Formula $X=x_{1},...,x_{L}$
\end_inset

 was generated by a parameterized model 
\begin_inset Formula $\theta$
\end_inset

, and has an hidden state sequence 
\begin_inset Formula $Y=y_{1},...,y_{L}$
\end_inset

 that was generated alongside it.
 In this generation process, a single observed variable is emitted per step
 of the model, and so the observed sequence is generated with the same length
 as the hidden Markov chain.
 The observed variables 
\begin_inset Formula $V_{1},...,V_{n}$
\end_inset

 are sampled from an emission distribution 
\begin_inset Formula $E_{i,j}=P(x_{t}=V_{j}|y_{t}=S_{i})$
\end_inset

, that is conditioned on the hidden state of the model.
 Similarly to the Markov model, the distribution to the first hidden state
 is marked as
\begin_inset Formula $\pi$
\end_inset

 and the transition distribution is marked as 
\begin_inset Formula $T$
\end_inset

.
\end_layout

\begin_layout Standard
For example, assuming the DNA are composed of genes enhancers and background
 regions, with each having different nucleotide frequency, then we can say
 that the DNA sequence was generated by a HMM with underlying sequence of
 4 hidden states: gene, promoter, enhancer and background where each has
 its own nucleotide frequency.
 The emitted observed DNA sequence X is determined by the underlying hidden
 sequence Y that describes the
\begin_inset Quotes eld
\end_inset

mode
\begin_inset Quotes erd
\end_inset

 of the sequence in each position.
\begin_inset Newline newline
\end_inset


\end_layout

\begin_layout Standard
\begin_inset Graphics
	filename Figures/HMM.jpg
	scale 60

\end_inset


\end_layout

\begin_layout Subparagraph*
[tommy: show an example of two states machine, moving between the states,
 generating a sequence that is 1 state then moving to another state an coming
 back.
 also more about the likelihood 
\begin_inset Formula $L=P(x)=\sum_{Y}p(Y|X)P(Y)$
\end_inset

]
\end_layout

\begin_layout Standard

\series bold
\emph on
Figure: a HMM chain with length of 5.
 The underlying hidden states (up) are generated in a Marokov process and
 each state emits an observed variable, sampled from the state's emission
 probability.
\end_layout

\begin_layout Paragraph*

\bar under
HMM Limitations
\end_layout

\begin_layout Standard
[preposition, motivation, regularization]
\end_layout

\begin_layout Standard
Although HMM is simple and efficient, applying it on DNA sequences has a
 major disadvantage which is the inherit Markovian lack-of-memory property.
 This property means that on every step of the model, the next state is
 dependent only on the previous state, without further history consideration.
 For the task of emitting a motif, where each position has a different emission
 distribution depending on the location in the motif, a HMM model would
 need to contain different hidden states per position in the motif.
 This means that for an HMM to be able to emit even a small number of short
 motifs, it needs to hold a large number of states that require learning
 a large number of parameters, e.g.
 for the ability to emit 50 motifs of length 5, an HMM needs to have over
 60,000 parameters.
 Furthermore, the enhancer modeling task at hand is even more complex, since
 we would like to model multiple enhancers and backgrounds states, each
 having different probability of emitting motifs and unique k-order emission
 distribution when not in those motifs.
 For our data structure prior assumption the required number of model's
 parameters would have been about 
\begin_inset Formula $10^{7}$
\end_inset

, large enough to introduce problems such as unfeasible memory complexity
 and overfitting the fixed-sized genome data.
 
\end_layout

\begin_layout Standard
A common way to avoid overfitting the data when training machine learning
 models is regularization the model's parameters, a method we can incorporate
 while training HMM as well.
 A possible regularization is  constraint on the learned transition matrix
 T to be sparse.
 Similarly, the proposed HOP-HMM addresses both the memory issue and the
 overfitting issue while remaining equivalent to a regularized HMM with
 a large number of states.
 Namely, most of the transition probabilities are fixed to zero and therefore
 never stored in memory, and some of the emission probabilities are predetermine
d and are fixed during the training\SpecialChar endofsentence
.
 This allows us to learn a model with the enhancer prior assumptions of
 motifs and high-order emission without overfitting, and with reasonable
 memory complexity.
\end_layout

\begin_layout Standard
There is a tradeoff regarding the use of high-order emission compared to
 high-order transition, or in other words whether the transition or the
 emission probability is conditional to the previous states and emissions.
 Both can emit sequences with structure of higher order, for example both
 can describe a sequence where C will never come after T, as well as sequences
 containing kmers, etc.
 The high-order transition allows more freedom and specificity in the shape
 of a per-location probability of a transition to another, unrelated state
 depending on previous states, with the cost of more parameters.
 This transition seemed unnecessary since we assumed the transition to another
 state, or enhancer in our case, is only loosely conditional to the last
 several emissions.
\end_layout

\begin_layout Subsection*

\bar under
HOP-HMM
\end_layout

\begin_layout Subparagraph*
[tommy: add here gentle intro: seq with single motif.
 figure of automat, figure of seq.
 then do same with multi motifs.
 then do with multi layers.
 Also a part about PWMs]
\end_layout

\begin_layout Standard
Here we present HOP-HMM, a variant model of HMM, that is well fitted to
 utilize the structure of enhancers containing TFBSs inside them.
 The base feature of HOP-HMM is its two types of hidden states: PWM sub-state,
 and base-states, where each type of sequence type is represented by a 
\begin_inset Quotes eld
\end_inset

layer
\begin_inset Quotes erd
\end_inset

 or row of states in figure 1, a base-state that can transfer into one of
 it's PWM sub-states.
 When generating the DNA sequence, the model travel between the layers and
 creates a sequence built as a mix of several types with different motif
 frequencies.
 Each layer on its own creates a series of nucleotides emitted from the
 base-state, while most layers represent an enhancer types, some layers
 are predefined as background sequences, and their probability to transfer
 into a sub-state is restricted.
\end_layout

\begin_layout Standard
We use two indices to describe a hidden state, one for its layer index and
 one to indicate whether it's a base-states or a sub-states.
 In a model with m layers and k PWMs, we mark the j'th layer base-state
 as 
\begin_inset Formula $(j,0)$
\end_inset

 and its l'th sub-state as 
\begin_inset Formula $(j,l)$
\end_inset

 for 
\begin_inset Formula $l\in[k]$
\end_inset

, in total there are m base-states and 
\begin_inset Formula $m\cdot k$
\end_inset

 sub-states.
 
\end_layout

\begin_layout Subparagraph*
[Tommy: in the figure, add an indication for a different emission of the
 states in the figure ]
\end_layout

\begin_layout Standard
\begin_inset Graphics
	filename Figures/Slide1.eps
	scale 50
	clip

\end_inset


\end_layout

\begin_layout Standard

\series bold
\shape italic
Figure 1: The hidden variable states graph of the HOP-HMM.
 Each row represents a sequence type, where the left hexagons are base-states
 and the circles to the right are their sub-states.
 Not all transitions are possible, and moving between the layers is possible
 only by a base-state to base-state transition.
 
\end_layout

\begin_layout Standard
A model that contains states that emit motifs sampled from PWMs was previously
 demonstrated (as described in [Kaplan et al.
 2011]) and as such it is considered a generalized hidden Markov model (gHMM)
 is a variant of HMM in which states may emit multiple letters.other states
 that emit letters depending on previous letters in the observed sequence.
 Similarly to HMM, it assumes an underlying hidden sequence is present that
 are sampled from a Markov chain.
\end_layout

\begin_layout Standard

\series bold
\bar under
Emission
\end_layout

\begin_layout Standard
On initialization the HOP-HMM is given k PWMs 
\begin_inset Formula $W_{1},W_{2},...,W_{k}$
\end_inset

 that remain fixed and aren't learned during training.
 The PWMs are each shared with m sub-states, e.g.
 the PWM 
\begin_inset Formula $W_{l}$
\end_inset

, where 
\begin_inset Formula $l\in[k]$
\end_inset

, is shared between subs-states 
\begin_inset Formula $(1,l),(2,l),...,(m,l)$
\end_inset

 and is used for the sub-state emission sampling.
 The PWMs vary in their column amounts (as the different TFBSs vary in length),
 where each column represents a nucleotide distribution at that position.
 When the model enters a sub-state, it emits a motif by sampling from a
 PWM column by column independently, as described in Figure 3.
\end_layout

\begin_layout Standard
\begin_inset Graphics
	filename Figures/Slide3.JPG
	scale 50

\end_inset

[tommy: remove the indices?]
\end_layout

\begin_layout Standard

\series bold
\shape italic
Figure 3: PWM emission of sub-states.
 PWMs are a set of per-position nucleotide emission probabilities, that
 are sampled independently to create a TFBS motif.
\end_layout

\begin_layout Standard
The base-states, denoted as 
\begin_inset Formula $(1,0),(2,0),...,(m,0)$
\end_inset

, are responsible for the emission of inter-TFBS parts of the enhancers
 lacking long motifs.
 Similarly to regular states in HMM, base-states emit single nucleotides,
 where their emission is conditional on the previous of letters emitted
 in the DNA sequence.
 The emission from base-states is done by sampling a nucleotide from the
 distributions stored in E tensor.
 E dimension is o+1, and its size is 
\begin_inset Formula $\text{ }m\times4\times4\times...\times4$
\end_inset

 (with o fours) and its values are describe the emission probability 
\begin_inset Formula $E_{j,x_{t-o+1},x_{t-o+2},...,x_{t}}=P\left(x_{t}|y_{t}=(j,0),x_{t-o+1},...,x_{t-1}\right)$
\end_inset

, meaning that when 
\begin_inset Formula $x_{t}$
\end_inset

 is sampled by the model, the preceding 
\begin_inset Formula $o-1$
\end_inset

 observed variables are used as indices of the tensor for getting emission
 probability vector 
\begin_inset Formula $E_{j,x_{t-o+1},x_{t-o+2},...,x_{t-1},*}$
\end_inset

.
 For the first variables emitted in the sequence, the missing dimensions
 of the preceding variables are summed to form the probability vector, e.g.
 if 
\begin_inset Formula $t=o-1$
\end_inset

, a single variable is missing for emitting 
\begin_inset Formula $x_{t}$
\end_inset

 and the distribution used for emission sampling is 
\begin_inset Formula $\sum_{i\in[4]}\frac{E_{j,i,x_{1},...,x_{t-1}}}{4}$
\end_inset

.
\begin_inset Newline newline
\end_inset


\end_layout

\begin_layout Standard
\begin_inset Graphics
	filename Figures/Slide2.JPG
	scale 50
	clip

\end_inset


\end_layout

\begin_layout Standard

\series bold
\shape italic
Figure 2: high-order emission of base-states.
 Each emission is dependent on the hidden base-state and o-1 previous observatio
ns.
\series default
\shape default

\begin_inset Newline newline
\end_inset


\end_layout

\begin_layout Paragraph*

\series bold
\bar under
Transition
\end_layout

\begin_layout Standard
In HOP-HMM, the first hidden state in a sequence can only be a base-state.
 The first base-state, as in HMM, is chosen by sampling from 
\begin_inset Formula $\pi,$
\end_inset

 a probability vector 
\begin_inset Formula $\pi_{j}=P(y_{1}=(j,0))$
\end_inset

.
 Once in a base-state, the model can only transit into a small subset of
 states, and since most of the possible transition are not allowed, a single
 transition matrix from all states to all states would be sparse.
 Instead, as described in figure 4, we hold only the possible transition
 probabilities in two matrices, representing the two types of allowed transition
s: 
\end_layout

\begin_layout Standard
T for base-state to base-state transitions, a 
\begin_inset Formula $m\times m$
\end_inset

 matrix where 
\begin_inset Formula $T_{j_{1},j_{2}}=P\left(y_{t+1}=(j_{2},0)|y_{t}=(j_{1},0)\right)$
\end_inset

.
\end_layout

\begin_layout Standard
G for base-state to sub-state transitions a 
\begin_inset Formula $m\times k$
\end_inset

 matrix where 
\begin_inset Formula $G_{j,l}=P\left(y_{t+1:t+|W_{l}|}=(j,l)|y_{t}=(j,0)\right)$
\end_inset

.
 
\end_layout

\begin_layout Standard
When in a base-state, after the single observable variable emission, the
 model samples its next state from a probability vector that is a concatenation
 of a row in T and a row in G.
 If a sub-state is chosen, after the sub-state's motif emission, the model
 returns back to the base-state to emit another single observable variable
 and so on.
\begin_inset Newline newline
\end_inset


\begin_inset Newline newline
\end_inset


\begin_inset Graphics
	filename Figures/HOP-HMM transition matrices.png
	scale 50

\end_inset


\end_layout

\begin_layout Standard

\series bold
\emph on
Figure 4: a compact way to hold the allowed transition probabilities.
 Concatenation of a row in T and G holds the probability of the next state
 given the current one.
\end_layout

\begin_layout Standard

\series bold
\bar under
HOP-HMM Denotations
\end_layout

\begin_layout Standard
For clarity, the denotations used for the HOP-HMM algorithm:
\end_layout

\begin_layout Standard
o - order of base-state emission, emit distribution of variable depends
 on 
\begin_inset Formula $o-1$
\end_inset

 last variables (hyper parameter)
\end_layout

\begin_layout Standard
m - number of base-states (hyper parameter)
\end_layout

\begin_layout Standard
k - number of sub-states for each base-state (hyper parameter)
\end_layout

\begin_layout Standard
L - length of the DNA sequence (data given to the algorithm)
\end_layout

\begin_layout Standard
\begin_inset Formula $Y=y_{1},...,y_{L}$
\end_inset

 - hidden states sequence (unknown feature of the data, estimated by the
 Viterbi algorithm)
\end_layout

\begin_layout Standard
\begin_inset Formula $y_{i}\in[m]\times[k]\cup0$
\end_inset

 - hidden state (unknown feature of the data, estimated by the Viterbi algorithm
)
\end_layout

\begin_layout Standard
\begin_inset Formula $X=x_{1},...,x_{L}$
\end_inset

 - observable variables sequence, which is the DNA sequence (data given
 to the algorithm)
\end_layout

\begin_layout Standard
\begin_inset Formula $x_{i}\in\{A,C,G,T\}$
\end_inset

 - observable variable, nucleotide (data given to the algorithm)
\end_layout

\begin_layout Standard
\begin_inset Formula $W_{i}$
\end_inset

 - the PWM of the i'th sub-state of all base-states in the model (hyper
 parameter)
\begin_inset Newline newline
\end_inset


\end_layout

\begin_layout Part*

\bar under
Baum-Welch Algorithm
\end_layout

\begin_layout Subparagraph*
[Tommy: Add, viterbi is not always the best way, since it has to be correct
 state seq, and chooses only one best state seq.
 This could not correlate to the sum of possible passes through a state
 via multiple paths, and there for be very different from the real state
 seq.]
\end_layout

\begin_layout Subparagraph*
In the maximal-likelihood estimation problem before us, we have the observed
\begin_inset Formula $X$
\end_inset

 and we would ultimately like find the parameters that maximize the likelihood
 of it,
\begin_inset Formula 
\[
\theta^{*}=argmax_{\theta}L\left(\theta|X\right)
\]

\end_inset


\end_layout

\begin_layout Standard
This likelihood function 
\begin_inset Formula $L\left(\theta|X\right)$
\end_inset

, also called the incomplete-data likelihood function, could be written,
 using the total probability law, as:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{equation}
L\left(\theta|X\right)=P(X|\theta)=\sum_{y\in\left[m\right]^{N}}P\left(X,y|\theta\right)
\end{equation}

\end_inset


\end_layout

\begin_layout Standard
The probability on the right side 
\begin_inset Formula $P(X,Y|\theta)=L(\theta|X,Y)$
\end_inset

 is called the complete-data likelihood function, and in the case of the
 basic HMM it is calculated by:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{equation}
P(X,y|\theta)=\pi_{y_{1}}E_{y_{1},x_{1}}\prod_{i=2}^{N}T_{y_{i-1},y_{i}}E_{y_{i},x_{i}}
\end{equation}

\end_inset


\end_layout

\begin_layout Standard
Note: while in our variation HOP-HMM (describe in later chapters),
\end_layout

\begin_layout Subparagraph*
[TODO: write specific chapter numbers] 
\end_layout

\begin_layout Standard
we assume different transitions and emissions hence requires different calculati
on, but otherwise the rest of the EM algorithm holds.
\end_layout

\begin_layout Standard
Unfortunately, optimizing or calculating the incomplete-data likelihood
 in (1) involves a summation of exponential-by-N elements (N is the length
 of the DNA sequence), which is infeasible.
 Instead, the strategy of the EM algorithm is to optimize the expected value
 of the complete-data log-likelihood 
\begin_inset Formula $log\left(P\left(X,Y|\theta^{'}\right)\right)$
\end_inset

 where 
\begin_inset Formula $\theta^{'}$
\end_inset

 is the model's parameters from previous EM iteration (or guessed parameters
 in the first iteration) and while assuming a fixed observed X, as it is
 our DNA sequence.
 For this task we can formally define our target function Q:
\begin_inset Formula 
\begin{equation}
Q\left(\theta,\theta^{'}\right)=E_{Y}\left[log\left(P\left(X,Y|\theta\right)\right)|X,\theta^{'}\right]=\sum_{y\in\left[m\right]^{N}}log\left(P\left(X,y|\theta\right)\right)P\left(X,y|\theta^{'}\right)
\end{equation}

\end_inset


\end_layout

\begin_layout Standard
Note that here, E is expressing an expected value and not the HMM emission
 probability.
 Every EM iteration is built of two parts: the E-step and the M-step.
 In the E-step we calculate the probabilities needed for the maximization
 of Q and in the M-step we infer the 
\begin_inset Formula $\theta$
\end_inset

 that maximizes it.
 Although this seemingly still requires an exponential summation, we can
 use a dynamic programming approach to overpass it, with the cost of 
\begin_inset Formula $O(N\cdot m)$
\end_inset

 memory usage.
\end_layout

\begin_layout Standard
Using equations (2) and (3) allows us to split the Q function to three independe
nt parts.
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{align*}
Q\left(\theta,\theta^{'}\right)= & \sum_{y\in\left[m\right]^{N}}log\pi_{y_{1}}\cdot P\left(X,y|\theta^{'}\right)\\
+ & \sum_{y\in\left[m\right]^{N}}\left(\sum_{t\in2...L}logT_{y_{t-1},y_{t}}\right)\cdot P\left(X,y|\theta^{'}\right)\\
+ & \sum_{y\in\left[m\right]^{N}}\left(\sum_{t\in[L]}logE_{y_{t},x_{t}}\right)\cdot P\left(X,y|\theta^{'}\right)
\end{align*}

\end_inset


\end_layout

\begin_layout Standard
then by manipulating the summation and the state sequence cases could be
 simplified to
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{align*}
Q\left(\theta,\theta^{'}\right)= & \sum_{j\in[m]}log\pi_{j}\cdot P\left(X,y_{1}=j|\theta^{'}\right)\\
+ & \sum_{t\in2...L}\sum_{j_{1},j_{2}\in[m]}logT_{j_{1},j_{2}}\cdot P\left(X,y_{t-1}=j_{1},y_{t}=j_{2}|\theta^{'}\right)\\
+ & \sum_{t\in[L]}\sum_{j\in[m]}logE_{j,x_{t}}\cdot P\left(X,y_{t}=j|\theta^{'}\right)
\end{align*}

\end_inset


\end_layout

\begin_layout Standard
Each of the three parts could be derived and maximized independently using
 a Lagrange multipliers, under the following probability constrains:
\end_layout

\begin_layout Standard
\begin_inset Formula $\sum_{j\in[m]}\pi_{j}=1$
\end_inset


\end_layout

\begin_layout Standard
\begin_inset Formula $\sum_{j_{2}\in[m]}T_{j_{1},j_{2}}=1$
\end_inset

 for all 
\begin_inset Formula $j_{1}\in[m]$
\end_inset


\end_layout

\begin_layout Standard
\begin_inset Formula $\sum_{b\in[n]}E_{j,b}=1$
\end_inset

 for all 
\begin_inset Formula $j\in[n]$
\end_inset


\end_layout

\begin_layout Standard
where m is the number of different hidden states and n is the number of
 different observed variables (4 in our case of DNA)
\end_layout

\begin_layout Standard
The first part is maximized using Lagrange multiplier
\begin_inset Formula $\lambda$
\end_inset

:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
\frac{\partial}{\partial\pi_{j}}\left(\sum_{j\in[m]}log\pi_{j}P\left(X,y_{1}=j|\theta^{'}\right)+\lambda\left(\sum_{j\in[m]}\pi_{j}-1\right)\right)=0
\]

\end_inset


\end_layout

\begin_layout Standard
we derive the equations and get 
\begin_inset Formula $\frac{P\left(X,y_{1}=j|\theta^{'}\right)}{\pi_{j}}=-\lambda$
\end_inset

 then sum them to receive 
\begin_inset Formula $\lambda=P\left(X|\theta^{'}\right)$
\end_inset

 and assign it and deduce:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{equation}
\pi_{j}=\frac{P\left(X,y_{1}=j|\theta^{'}\right)}{P\left(X|\theta^{'}\right)}=P\left(y_{1}=j|X,\theta^{'}\right)
\end{equation}

\end_inset


\end_layout

\begin_layout Standard
we follow this method similarly in the second and third parts, from which
 we receive:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{equation}
T_{j_{1},j_{2}}=\frac{\sum_{t\in2...L}P\left(X,y_{t-1}=j_{1},y_{t}=j_{2}|\theta^{'}\right)}{\sum_{t\in2...L}P\left(X,y_{t-1}=j_{1}|\theta^{'}\right)}=\frac{\sum_{t\in2...L}P\left(y_{t-1}=j_{1},y_{t}=j_{2}|X,\theta^{'}\right)}{\sum_{t\in2...L}P\left(y_{t-1}=j_{1}|X,\theta^{'}\right)}
\end{equation}

\end_inset


\end_layout

\begin_layout Standard
and
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{equation}
E_{j,b}=\frac{\sum_{t\in[L]}P\left(X,y_{t}=j|\theta^{'}\right)\boldsymbol{1}_{b}(x_{t})}{\sum_{t\in[L]}P\left(X,y_{t}=j|\theta^{'}\right)}=\frac{\sum_{t\in[L]}P\left(y_{t}=j|X,\theta^{'}\right)\boldsymbol{1}_{b}(x_{t})}{\sum_{t\in[L]}P\left(y_{t}=j|X,\theta^{'}\right)}
\end{equation}

\end_inset


\end_layout

\begin_layout Standard
where 
\begin_inset Formula $\boldsymbol{1}_{b}(x_{t})=\begin{cases}
1 & b=x_{t}\\
0 & otherwise
\end{cases}$
\end_inset

 
\end_layout

\begin_layout Standard
After stating the intentions of each EM iteration in (4), (5) and (6), we
 now need to calculated them in order to successfully learn the HMM parameters.
 Specifically, notice that to resolve all the parameters update states in
 (4), (5) and (6), it is enough to calculate the two terms during the E-step:
 
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{equation}
P\left(y_{t}=j|X,\theta^{'}\right)
\end{equation}

\end_inset


\begin_inset Formula 
\begin{equation}
P\left(y_{t-1}=j_{1},y_{t}=j_{2}|X,\theta^{'}\right)
\end{equation}

\end_inset


\end_layout

\begin_layout Standard
We will calculate these terms by using the Forward-Backward algorithm.
 The Forward-Backward algorithm (Rabiner, 1989) is a method to dynamically
 calculate two matrices, 
\begin_inset Formula $\alpha$
\end_inset

 and 
\begin_inset Formula $\beta,$
\end_inset

 both of size 
\begin_inset Formula $m\times L$
\end_inset

.
 The forward probabilities matrix 
\begin_inset Formula $\alpha$
\end_inset

 holds: 
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
\alpha_{j,t}=P(y_{t}=j,x_{1:t})
\]

\end_inset


\end_layout

\begin_layout Standard
which is, in other words, that 
\begin_inset Formula $\alpha_{j,t}$
\end_inset

 is the probability that a sequence 
\begin_inset Formula $x_{1:t}$
\end_inset

 was emitted and the hidden states series ended with the hidden state j.
\end_layout

\begin_layout Standard
The calculation is done by iterating over 
\begin_inset Formula $t=1,2,...,L$
\end_inset

, in each iteration filling all 
\begin_inset Formula $j\in[m]$
\end_inset

 matrix cells as following:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{align*}
for\,t=1:\\
 & \alpha_{j,t}=\pi_{j}\cdot E_{j,x_{1}}\\
for\,t=2,...,L:\\
 & \alpha_{j,t}=\sum_{j'\in[m]}\alpha_{j',t-1}\cdot T_{j',j}\cdot E_{j,x_{t}}
\end{align*}

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
\alpha_{j,t}=P\left(y_{t}=j,x_{1:t}\right)=P\left(x_{t}|y_{t}=j,x_{1:t-1}\right)\cdot P\left(y_{t}=j,x_{1:t-1}\right)
\]

\end_inset


\end_layout

\begin_layout Standard
The building of the table is based on the HMM basic assumptions that each
 hidden state 
\begin_inset Formula $y_{t}$
\end_inset

 is dependent only on the previous one 
\begin_inset Formula $y_{t-1}$
\end_inset

 and that each observed variable 
\begin_inset Formula $x_{t}$
\end_inset

 is dependent only on its hidden state that emitted it 
\begin_inset Formula $y_{t}$
\end_inset

.
 
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
P\left(x_{t}|y_{t}=j,x_{1:t-1}\right)\cdot P\left(y_{t}=j,x_{1:t-1}\right)
\]

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
=P\left(x_{t}|y_{t}=j\right)\cdot\sum_{j'\in[m]}P\left(y_{t}=j,y_{t-1}=j',x_{1:t-1}\right)=
\]

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
=P\left(x_{t}|y_{t}=j\right)\cdot\sum_{j'\in[m]}P\left(y_{t}=j|y_{t-1}=j'\right)\cdot P\left(y_{t-1}=j',x_{1:t-1}\right)=
\]

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
=E_{j,x_{t}}\cdot\sum_{j'\in[m]}T_{j',j}\cdot\alpha_{j',t-1}
\]

\end_inset


\end_layout

\begin_layout Standard
The backwards probabilities matrix 
\begin_inset Formula $\beta$
\end_inset

 holds:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
\beta_{j,t}=P\left(x_{t+1:L}|y_{t}=j\right)
\]

\end_inset


\end_layout

\begin_layout Standard
which is the probability that a sequence 
\begin_inset Formula $x_{t+1:L}$
\end_inset

 was emitted given the hidden state at position t had value j.
\end_layout

\begin_layout Standard
Filling the 
\begin_inset Formula $\beta$
\end_inset

 is done in the opposite direction 
\begin_inset Formula $t=L,L-1,...,1$
\end_inset

, and for all 
\begin_inset Formula $j\in[m]$
\end_inset

 as following:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{align*}
for\,t=L:\\
 & \beta_{j,t}=\frac{1}{m}\\
for\,t=L-1,...,1:\\
 & \beta_{j,t}=\sum_{j'\in[m]}\beta_{j',t+1}\cdot T_{j,j'}\cdot E_{j',x_{t}}
\end{align*}

\end_inset


\end_layout

\begin_layout Standard
This matrix building process is similarly explained by:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
\beta_{j,t}=P\left(x_{t+1:L}|y_{t}=j\right)=\sum_{j'\in[m]}P\left(y_{t+1}=j',x_{t+1:L}|y_{t}=j\right)=
\]

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
=\sum_{j'\in[m]}P\left(x_{t+2:L}|y_{t}=j\right)\cdot P\left(x_{t+1}|y_{t}=j,y_{t+1}=j'\right)\cdot P\left(y_{t+1}=j'|y_{t}=j\right)=
\]

\end_inset


\begin_inset Formula 
\[
=\sum_{j'\in[m]}P\left(x_{t+2:L}|y_{t+1}=j'\right)\cdot P\left(x_{t+1}|y_{t+1}=j'\right)\cdot P\left(y_{t+1}=j'|y_{t}=j\right)=
\]

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
=\sum_{j'\in[m]}\beta_{j',t+1}\cdot E_{j',x_{t+1}}\cdot T_{j,j'}
\]

\end_inset


\end_layout

\begin_layout Standard
Having the forward and backward probability matrices 
\begin_inset Formula $\alpha$
\end_inset

 and 
\begin_inset Formula $\beta$
\end_inset

, we now have all that is needed to calculate 
\family roman
\series medium
\shape up
\size normal
\emph off
\bar no
\strikeout off
\uuline off
\uwave off
\noun off
\color none
(8) and (9), using once more the HMM conditional independence in our calculation
s.
 
\family default
\series default
\shape default
\size default
\emph default
\bar default
\strikeout default
\uuline default
\uwave default
\noun default
\color inherit
Here we used the fact that
\end_layout

\begin_layout Standard

\family roman
\series medium
\shape up
\size normal
\emph off
\bar no
\strikeout off
\uuline off
\uwave off
\noun off
\color none
We denote the terms values of (7) as 
\begin_inset Formula $\gamma$
\end_inset

, a matrix of size 
\begin_inset Formula $L\times m$
\end_inset

:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
\gamma_{t,j}=P\left(y_{t}=j|X,\theta^{'}\right)=\frac{P\left(y_{t}=j,X|\theta^{'}\right)}{P\left(X|\theta^{'}\right)}=\frac{P\left(X|y_{t}=j,\theta^{'}\right)\cdot P\left(y_{t}=j|\theta^{'}\right)}{P\left(X|\theta^{'}\right)}=
\]

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
=\frac{P\left(y_{t}=j,x_{1:t}\right)\cdot P\left(x_{t+1:L}|y_{t}=j\right)}{\sum_{j'\in[m]}P\left(y_{t}=j',x_{1:t}\right)\cdot P\left(x_{t+1:L}|y_{t}=j'\right)}=\frac{\alpha_{j,t}\cdot\beta_{j,t}}{\sum_{j'\in[m]}\alpha_{j',t}\cdot\beta_{j',t}}
\]

\end_inset


\end_layout

\begin_layout Standard

\family roman
\series medium
\shape up
\size normal
\emph off
\bar no
\strikeout off
\uuline off
\uwave off
\noun off
\color none
And we denote the values of (9) as 
\begin_inset Formula $\xi$
\end_inset

, a matrix of size 
\family default
\series default
\shape default
\size default
\emph default
\bar default
\strikeout default
\uuline default
\uwave default
\noun default
\color inherit

\begin_inset Formula $L-1\times m\times m$
\end_inset

 :
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
\xi_{t,j_{1},j_{2}}=P\left(y_{t-1}=j_{1},y_{t}=j_{2}|X,\theta^{'}\right)=\frac{P\left(y_{t-1}=j_{1},y_{t]}=j_{2},X|\theta^{'}\right)}{P\left(X|\theta^{'}\right)}=
\]

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
=\frac{P\left(y_{t-1}=j_{1},x_{1:t-1}\right)\cdot P\left(y_{t}=j_{2}|y_{t-1}=j_{1}\right)\cdot P\left(x_{t}|y_{t}=j_{2}\right)\cdot P\left(x_{t+1:L}|y_{t}=j_{2}\right)}{\sum_{j'\in[m]}P\left(y_{t}=j,x_{1:t}\right)\cdot P\left(x_{t+1:L}|y_{t}=j\right)}=
\]

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
=\frac{\alpha_{j_{1},t-1}\cdot T_{j_{1},j_{2},}\cdot E_{j_{2},x_{t}}\cdot\beta_{j',t}}{\sum_{j'\in[m]}\alpha_{j',t}\cdot\beta_{j',t}}
\]

\end_inset


\end_layout

\begin_layout Standard
 
\end_layout

\begin_layout Section*
Baum-Welch Algorithm for HOP-HMM
\end_layout

\begin_layout Standard
The Baum-Welch algorithm can be adjusted to infer the parameters of the
 HOP-HMM variant 
\begin_inset Formula $\theta=\{\pi,E,G,T\}$
\end_inset

 from a DNA sequence X.
 As in the regular Baum-Welch algorithm covered in the previous section,
 given a sequence X at each EM iteration we calculate and optimize a Q function:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{align*}
Q\left(\theta,\theta^{'}\right)= & \sum_{j\in[m]}log\pi_{j}\cdot P\left(X,y_{1}=(j,0)|\theta^{'}\right)\\
+ & \sum_{t\in2...L}\sum_{j_{1},j_{2}\in[m]}logT_{j_{1},j_{2}}\cdot P\left(X,y_{t-1}=(j_{1},0),y_{t}=(j_{2},0)|\theta^{'}\right)\\
+ & \sum_{t\in2...L}\sum_{j\in[m],l\in[k]}logG_{j,l}\cdot P\left(X,y_{t-1}=(j,0),y_{t}=(j,l)|\theta^{'}\right)\\
+ & \sum_{t\in[L]}\sum_{j\in[m]}logE_{j,x_{t}}\cdot P\left(X,y_{t}=(j,0)|\theta^{'}\right)\\
+ & \sum_{t\in[L]}\sum_{l\in[k]}logL_{W}(x_{t:t+|W_{l}|})\cdot P\left(X,y_{t:t+|W_{l}|}=(j,l)|\theta^{'}\right)
\end{align*}

\end_inset


\end_layout

\begin_layout Standard
where 
\begin_inset Formula $L_{W}(\overline{x})$
\end_inset

 is the likelihood of motif 
\begin_inset Formula $\overline{x}$
\end_inset

 to be emitted by PWM 
\begin_inset Formula $W$
\end_inset

: 
\begin_inset Formula $L_{M}(\overline{x})=P(\overline{x}|W)=\underset{i\in\{1,...,|\overline{x}|\}}{\prod}W_{\overline{x}_{i},i}$
\end_inset

.
 Note that the last component does not contain any part of 
\begin_inset Formula $\theta$
\end_inset

, due to the fact that the PWMs are fixed, so it won't be optimized in the
 m-steps.
 
\end_layout

\begin_layout Subparagraph*
[TODO: consider with Tommy adding here the optimization of each of the four
 parts of Q using Lagrange multipliers]
\end_layout

\begin_layout Standard
The 
\begin_inset Formula $\theta$
\end_inset

 which optimizes Q, i.e.
 
\begin_inset Formula $\theta_{max}=argmax_{\theta}Q(\theta,\theta')$
\end_inset

, is built similarly as in the regular Baum-Welch algorithm, as following:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{equation}
\pi_{j}=\frac{P\left(X,y_{1}=(j,0)|\theta^{'}\right)}{P\left(X|\theta^{'}\right)}=P\left(y_{1}=(j,0)|X,\theta^{'}\right)
\end{equation}

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{equation}
T_{j_{1},j_{2}}=\frac{\sum_{t\in2...L}P\left(X,y_{t-1}=(j_{1},0),y_{t}=(j_{2},0)|\theta^{'}\right)}{\sum_{t\in2...L}P\left(X,y_{t-1}=(j_{1},0)|\theta^{'}\right)}=\frac{\sum_{t\in2...L}P\left(y_{t-1}=(j_{1},0),y_{t}=(j_{2},0)|X,\theta^{'}\right)}{\sum_{t\in2...L}P\left(y_{t-1}=(j_{1},0)|X,\theta^{'}\right)}
\end{equation}

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{equation}
G_{j,l}=\frac{\sum_{t\in2...L}P\left(X,y_{t-1}=(j,0),y_{t}=(j,l)|\theta^{'}\right)}{\sum_{t\in2...L}P\left(X,y_{t-1}=(j,0)|\theta^{'}\right)}=\frac{\sum_{t\in2...L}P\left(y_{t-1}=(j,0),y_{t}=(j,l)|X,\theta^{'}\right)}{\sum_{t\in2...L}P\left(y_{t-1}=(j,0)|X,\theta^{'}\right)}
\end{equation}

\end_inset


\end_layout

\begin_layout Standard
and
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{equation}
E_{j,b_{1},...,b_{o}}=\frac{\sum_{t\in o,...,L}P\left(X,y_{t}=(j,0)|\theta^{'}\right)\boldsymbol{1}_{b_{1},...,b_{o}}(x_{t-o+1,...,t})}{\sum_{t\in o,...,L}P\left(X,y_{t}=(j,0)|\theta^{'}\right)}=\frac{\sum_{t\in o,...,L}P\left(y_{t}=(j,0)|X,\theta^{'}\right)\boldsymbol{1}_{b_{1},...,b_{o}}(x_{t-o+1,...,t})}{\sum_{t\in o,...,L}P\left(y_{t}=(j,0)|X,\theta^{'}\right)}
\end{equation}

\end_inset


\end_layout

\begin_layout Standard
Hence to complete the EM iteration, the three missing components in (9),(10),(11
),(12) are:
\begin_inset Newline newline
\end_inset


\begin_inset Newline newline
\end_inset


\begin_inset Formula 
\begin{equation}
P\left(y_{t}=(j,0)|X,\theta^{'}\right)
\end{equation}

\end_inset


\begin_inset Formula 
\begin{equation}
P\left(y_{t-1}=(j_{1},0),y_{t}=(j_{2},0)|X,\theta^{'}\right)
\end{equation}

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{equation}
P\left(y_{t-1}=(j,0),y_{t}=(j,l)|X,\theta^{'}\right)
\end{equation}

\end_inset


\end_layout

\begin_layout Standard
For the calculation of these probabilities, we will need to calculate the
 forward and backward probabilities, and then few other auxiliary terms.
 The forward and backward probabilities are only for indicating the sequence
 entering-to and exiting-from base-states:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
\alpha_{j,t}=P\left(y_{t}=(j,0),x_{1:t}\right)
\]

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
\beta_{j,t}=P\left(x_{t+1:L}|y_{t}=(j,0)\right)
\]

\end_inset

We calculate 
\begin_inset Formula $\alpha$
\end_inset

 of size 
\begin_inset Formula $m\times L$
\end_inset

, iterating over 
\begin_inset Formula $t=1,2,...,L$
\end_inset

 as following:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{align*}
for\,t=1:\\
 & \alpha_{j,t}=\pi_{j}\cdot E_{j,x_{1}}\\
for\,t=o,...,L:\\
 & \alpha_{j,t}=\underset{\text{base-state transitions}}{\underbrace{\sum_{j'\in[m]}\alpha_{j',t-1}\cdot T_{j',j}\cdot E_{j,x_{t-o+1},...,x_{t}}}}\\
 & +\underset{\text{sub-state transitions}}{\underbrace{\sum_{l\in[k]\cap\{i|t>|W_{i}|+1\}}\alpha_{j,t-|W_{l}|-1}\cdot G_{j,l}\cdot L_{W_{l}}\left(x_{t-|W_{l}|},...,x_{t-1}\right)\cdot E_{j,x_{t-o+1},...,x_{t}}}}
\end{align*}

\end_inset

Note that when 
\begin_inset Formula $t<o$
\end_inset

 , part of the preceding observable variables are missing.
 Since E has 
\begin_inset Formula $o+1$
\end_inset

 dimensions, 
\begin_inset Formula $E_{j,x_{1},...,x_{t}}$
\end_inset

 is not well defined, so we define it here as 
\begin_inset Formula $E_{j,x_{1},...,x_{t}}=\underset{b_{1},...,b_{o-t}\in\{A,C,G,T\}}{\sum}\frac{1}{4^{o-t}}\cdot E_{j,b_{1},..,.b_{o-t},x_{1},...,x_{t}}$
\end_inset

 that is the expected probability upon possible preceding variables.
 We used the fact that 
\begin_inset Formula $P(A)=\sum_{b\in B}P(b)\cdot P(A|b)$
\end_inset

 and the assumption that the observable variables preceding the sequence
 came from a uniform distribution, 
\begin_inset Formula $P(x_{i})=\frac{1}{4}$
\end_inset

 where 
\begin_inset Formula $i<1$
\end_inset

.
\end_layout

\begin_layout Standard
For 
\begin_inset Formula $\beta$
\end_inset

 of size 
\begin_inset Formula $m\times L$
\end_inset

, we iterating over 
\begin_inset Formula $t=L,L-1,...,1$
\end_inset

 as following:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{align*}
for\,t=L:\\
 & \beta_{j,t}=\frac{1}{m}\\
for\,t=L-1,...,1:\\
 & \beta_{j,t}=\underset{\text{base-state transitions}}{\underbrace{\sum_{j'\in[m]}\beta_{j',t+1}\cdot E_{u,x_{t-o+2},...,x_{t+1}}\cdot T_{j,j'}}}\\
 & +\underset{\text{sub-state transitions}}{\underbrace{\sum_{l\in\{1,...,k\}}\beta_{j,t+|W_{l}|+1}\cdot L_{W_{l}}\left(x_{t+1},...,x_{t+|W_{l}|}\right)\cdot E_{j,x_{t-o+|W_{l}|+2},...,x_{t+|W_{l}|+1}}\cdot G_{j,l}}}
\end{align*}

\end_inset


\end_layout

\begin_layout Standard
Note that when 
\begin_inset Formula $t>L-|W_{l}|$
\end_inset

, there are missing observable variables to fully calculate the sub-state
 transition.
 In these situations this contribution of these component to the summation
 is zero, meaning our HOP-HMM as the behavior of avoiding a transition into
 a PWM sub-state when the PWM is too long to fit into the sequence X length.
\end_layout

\begin_layout Subparagraph*
[TODO: problem, E is peaking before the t when doing a high-order emission
 of the base-states]
\end_layout

\begin_layout Standard
\begin_inset Graphics
	filename Figures/HOP-EM forward Algorithm.png
	scale 50

\end_inset


\end_layout

\begin_layout Standard

\series bold
\emph on
Figure 5: In our version of the forward-backward algorithm, each of the
 
\begin_inset Formula $\alpha$
\end_inset

 and 
\begin_inset Formula $\beta$
\end_inset

 cells are filled from both the adjacent base-states transitions and the
 base-states preceding or proceeding the motifs emitted by the sub-states.
\begin_inset Newline newline
\end_inset


\end_layout

\begin_layout Standard

\series bold
\bar under
Forward Algorithm Proof
\end_layout

\begin_layout Standard
We will now explain why the described dynamic calculation result with 
\begin_inset Formula $\alpha_{j,t}=P\left(y_{t}=(j,0),x_{1:t}\right)$
\end_inset

 and 
\begin_inset Formula $\beta_{j,t}=P\left(x_{t+1:L}|y_{t}=(j,0)\right)$
\end_inset

, starting with the forward probabilities 
\begin_inset Formula $\alpha$
\end_inset

.
 From the law of total probability, the probability 
\begin_inset Formula $\alpha_{j,t}$
\end_inset

 is the sum of probabilities of all the possible transition that ended in
 the base-state (j,0):
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
\alpha_{j,t}=P\left(y_{t}=(j,0),x_{1:t}\right)=
\]

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
=\underset{\text{base-state transitions}}{\underbrace{\underset{j'\in[m]}{\sum}P\left(y_{t-1}=(j',0),y_{t}=(j,0),x_{1:t}\right)}}+\underset{\text{sub-state transitions}}{\underbrace{\underset{l\in\{1,...,k\}}{\sum}P\left(y_{t-|W_{l}|:t-1}=(j,l),y_{t-|W_{l}|-1}=(j,0),x_{1:t}\right)}}
\]

\end_inset


\end_layout

\begin_layout Standard
We could develop the right term of a sub-state transition using the chain
 rule:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{align*}
P\left(y_{t-|W_{l}|:t-1}=(j,l),y_{t-|W_{l}|-1}=(j,0),x_{1:t}\right)= & \,P\left(y_{t-|W_{l}|-1}=(j,0),x_{1:t-|W_{l}|-1}\right)\cdot\\
 & \cdot P\left(y_{t-|W_{l}|:t-1}=(j,l)|y_{t-|W_{l}|-1}=(j,0),x_{1:t-|W_{l}|-1}\right)\\
 & \cdot P\left(x_{t-|W_{l}|:t-1}|y_{t-|W_{l}|:t-1}=(j,l),y_{t-|W_{l}|-1}=(j,0),x_{1:t-|W_{l}|-1}\right)\\
 & \cdot P\left(x_{t}|y_{t}=(j,0),x_{1:t-1},y_{t-|W_{l}|:t-1}=(j,l),y_{t-|W_{l}|-1}=(j,0)\right)
\end{align*}

\end_inset


\end_layout

\begin_layout Standard
Because of 
\begin_inset Formula $x_{t}$
\end_inset

 is dependent on only 
\begin_inset Formula $y_{t}$
\end_inset

 (and also 
\begin_inset Formula $x_{t-o:t-1}$
\end_inset

 if 
\begin_inset Formula $y_{t}$
\end_inset

 is a base-state) and 
\begin_inset Formula $y_{t}$
\end_inset

 is dependent on only 
\begin_inset Formula $y_{t-1}$
\end_inset

, we can simplify the probabilities:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{align*}
P\left(y_{t-|W_{l}|:t-1}=(j',l),y_{t-|W_{l}|-1}=(j,0),x_{1:t}\right)= & \,P\left(y_{t-|W_{l}|-1}=(j,0),x_{1:t-|W_{l}|-1}\right)\\
 & \cdot P\left(y_{t-|W_{l}|:t-1}=(j,l)|y_{t-|W_{l}|-1}=(j,0)\right)\\
 & \cdot P\left(x_{t-|W_{l}|:t-1}|y_{t-|W_{l}|:t-1}=(j,l)\right)\\
 & \cdot P\left(x_{t}|y_{t}=(j,0),x_{t-o:t-1}\right)
\end{align*}

\end_inset


\end_layout

\begin_layout Standard
We can now replace the received terms with the components of
\begin_inset Formula $\theta$
\end_inset

 and with already filled
\begin_inset Formula $\alpha$
\end_inset

 cells:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
P\left(y_{t-|W_{l}|:t-1}^{i}=(j,l),y_{t-|W_{l}|-1}^{i}=(j,0),x_{1:t}^{i}\right)=\alpha_{j,t-|W_{l}|-1}\cdot G_{j,l}\cdot L_{W_{l}}\left(x_{t-|W_{l}|},...,x_{t-1}\right)\cdot E_{j,x_{t-o+1},...,x_{t}}
\]

\end_inset


\begin_inset Newline newline
\end_inset


\end_layout

\begin_layout Standard
This process is similar to the base-state transition.
 Using the chain rule:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
P\left(y_{t-1}=(j',0),y_{t}=(j,0),x_{1:t}\right)=P\left(y_{t-1}=(j',0),x_{1:t-1}\right)\cdot P\left(y_{t}=(j,0)|y_{t-1}=(j',0),x_{1:t-1}\right)\cdot P\left(x_{t}|y_{t}=(j,0),y_{t-1}=(j',0),x_{1:t-1}\right)=
\]

\end_inset


\end_layout

\begin_layout Standard
Using the conditional independencies to simplify the probabilities:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
=P(y_{t-1}=(j',0),x_{1:t-1})\cdot P\left(y_{t}=(j,0)|y_{t-1}=(j',0)\right)\cdot P\left(x_{t}|y_{t}=(j,0),x_{1:t-1}\right)=\alpha_{j',t-1}\cdot T_{j',j}\cdot E_{j,x_{t-o+1},...,x_{t}}
\]

\end_inset


\end_layout

\begin_layout Standard

\series bold
\bar under
Backward Algorithm Proof
\end_layout

\begin_layout Standard
For the backward probabilities 
\begin_inset Formula $\beta,$
\end_inset

 the explanation is similar.
 Using the law of total probability:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{align*}
\beta_{j,t}= & P\left(x_{t+1:L}|y_{t}=(j,0)\right)=\\
= & \underset{\text{base-state transitions}}{\underbrace{\sum_{j'\in[m]}\beta_{j',t+1}\cdot E_{u,x_{t-o+2},...,x_{t+1}}\cdot T_{j,j'}}}\\
+ & \underset{\text{sub-state transitions}}{\underbrace{\sum_{l\in\{1,...,k\}}\beta_{j,t+|W_{l}|+1}\cdot L_{W_{l}}\left(x_{t+1},...,x_{t+|W_{l}|}\right)\cdot E_{j,x_{t-o+|W_{l}|+2},...,x_{t+|W_{l}|+1}}\cdot G_{j,l}}}
\end{align*}

\end_inset

 
\end_layout

\begin_layout Standard
Law of total probability:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
P\left(x_{t+1:L}|y_{t}=(j,0)\right)=
\]

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
=\underset{\text{base-state transition}}{\underbrace{\sum_{j'}P\left(y_{t+1}=(j',0),x_{t+1:L}|y_{t}=(j,0)\right)}}+\underset{\text{sub-state transition}}{\underbrace{\sum_{l}P\left(y_{t+1:t+\left|W_{l}\right|}=(j,l),y_{t+\left|W_{l}\right|+1}=(j,0),x_{t+1:L}|y_{t}=(j,0)\right)}}
\]

\end_inset


\end_layout

\begin_layout Standard
For the base-state transition term,using the chain rule:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{align*}
P\left(y_{t+1}=(j',0),x_{t+1:L}|y_{t}=(j,0)\right)= & P\left(x_{t+2:L}|y_{t+1}=(j',0),y_{t}=(j,0),x_{t+1}\right)\\
 & \cdot P\left(x_{t+1}|y_{t+1}=(j',0),y_{t}=(j,0)\right)\\
 & \cdot P\left(y_{t+1}=(j',0)|y_{t}=(j,0)\right)
\end{align*}

\end_inset


\end_layout

\begin_layout Standard
Using the conditional independencies to simplify the probabilities:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{align*}
P\left(y_{t+1}=(j',0),x_{t+1:L}|y_{t}=(j,0)\right)= & P\left(x_{t+2:L}|y_{t+1}=(j',0)\right)\\
 & \cdot P\left(x_{t+1}|y_{t+1}=(j',0),y_{t}=(j,0)\right)\\
 & \cdot P\left(y_{t+1}=(j',0)|y_{t}=(j,0)\right)=\\
= & \beta_{j',t+1}\cdot E_{u,x_{t-o+2},...,x_{t+1}}\cdot T_{j,j'}
\end{align*}

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Formula $P\left(y_{t-|W_{l}|:t-1}=(j',l),y_{t-|W_{l}|-1}=(j,0),x_{1:t}\right)=\,P\left(y_{t-|W_{l}|-1}=(j,0),x_{1:t-|W_{l}|-1}\right)\cdot\cdot P\left(y_{t-|W_{l}|:t-1}=(j,l)|y_{t-|W_{l}|-1}=(j,0),x_{1:t-|W_{l}|-1}\right)\cdot P\left(x_{t-|W_{l}|:t-1}|y_{t-|W_{l}|:t-1}=(j,l),y_{t-|W_{l}|-1}=(j,0),x_{1:t-|W_{l}|-1}\right)\cdot P\left(x_{t}|y_{t}=(j,0),x_{1:t-1},y_{t-|W_{l}|:t-1}=(j,l),y_{t-|W_{l}|-1}=(j,0)\right)$
\end_inset


\end_layout

\begin_layout Standard
For the sub-state transition term, using the chain rule:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{align*}
P\left(y_{t+1:t+\left|W_{l}\right|}=(j,l),y_{t+\left|W_{l}\right|+1}=(j,0),x_{t+1:L}|y_{t}=(j,0)\right)= & P\left(x_{t+\left|W_{l}\right|+2:L}|x_{t+1:t+\left|W_{l}\right|+1},y_{t+1:t+\left|W_{l}\right|}=(j,l),y_{t+\left|W_{l}\right|+1}=(j,0),y_{t}=(j,0)\right)\\
 & \cdot P\left(x_{t+\left|W_{l}\right|+1}|x_{t+1:t+\left|W_{l}\right|},y_{t+1:t+\left|W_{l}\right|}=(j,l),y_{t+\left|W_{l}\right|+1}=(j,0),y_{t}=(j,0)\right)\\
 & \cdot P\left(x_{t+1:t+\left|W_{l}\right|}|y_{t+1:t+\left|W_{l}\right|}=(j,l),y_{t+\left|W_{l}\right|+1}=(j,0),y_{t}=(j,0)\right)\\
 & \cdot P\left(y_{t+1:t+\left|W_{l}\right|}=(j,l),y_{t+\left|W_{l}\right|+1}=(j,0)|y_{t}=(j,0)\right)
\end{align*}

\end_inset


\end_layout

\begin_layout Standard
Using the conditional independencies to simplify the probabilities:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{align*}
P\left(y_{t+1:t+\left|W_{l}\right|}=(j,l),y_{t+\left|W_{l}\right|+1}=(j,0),x_{t+1:L}|y_{t}=(j,0)\right)= & P\left(x_{t+\left|W_{l}\right|+2:L}|y_{t+\left|W_{l}\right|+1}=(j,0)\right)\\
 & \cdot P\left(x_{t+\left|W_{l}\right|+1}|y_{t+\left|W_{l}\right|+1}=(j,0)\right)\\
 & \cdot P\left(x_{t+1:t+\left|W_{l}\right|}|y_{t+1:t+\left|W_{l}\right|}=(j,l)\right)\\
 & \cdot P\left(y_{t+1:t+\left|W_{l}\right|}=(j,l),y_{t+\left|W_{l}\right|+1}=(j,0)|y_{t}=(j,0)\right)=\\
= & \beta_{j,t+|W_{l}|+1}\cdot E_{j,x_{t-o+|W_{l}|+1},...,x_{t+|W_{l}|+1}}\cdot L_{W_{l}}\left(x_{t+1},...,x_{t+|W_{l}|}\right)\cdot G_{j,l}
\end{align*}

\end_inset


\end_layout

\begin_layout Standard

\series bold
\bar under
Auxiliary Probabilities 
\end_layout

\begin_layout Standard
Using the forward and the backward probability matrices, we are ready to
 calculate the auxiliary probabilities, that will help us calculate (9)
 (10) (11).
\end_layout

\begin_layout Standard
First, we calculate the probability of sub-state starting at a given position,
 with the given sequence X.
 We denote it as 
\begin_inset Formula $\psi$
\end_inset

 as a matrix of size 
\begin_inset Formula $m\times k\times L$
\end_inset

 
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{align*}
\psi_{j,l,t}= & P\left(y_{t}=(j,0),y_{t+1}=(j,l),X\right)=\\
= & P\left(y_{t}=(j,0),x_{1:t}\right)\cdot P\left(y_{t+1}=(j,l)|y_{t}=(j,0)\right)\\
 & \cdot P\left(x_{t+1:t+\left|W_{l}\right|}|y_{t+1:t+\left|W_{l}\right|}=(j,l)\right)\\
 & \cdot P\left(x_{t+\left|W_{l}\right|+1}|y_{t+\left|W_{l}\right|+1}=(j,0)\right)\\
 & \text{\cdot}P\left(x_{t+|W_{l}|+2:L}|y_{t+|W_{l}|+1}=(j,0)\right)=\\
= & \alpha_{j,t}\cdot G_{j,l}\cdot L_{W_{l,}}\left(x_{t+1},...,x_{t+|W_{l}|}\right)\cdot E_{j,x_{t+|W_{l}|-o+2},...,x_{t+|W_{l}|+1}}\cdot\beta_{j,t+|W_{l}|+1}
\end{align*}

\end_inset


\end_layout

\begin_layout Standard
Useful terms we can achieve with 
\begin_inset Formula $\psi$
\end_inset

 is the probability of a sub-state at a given position with the sequence
 X, and the probability of X:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
P\left(y_{t}=(j,l),X\right)=\underset{s\in\left[|W_{l}|\right]}{\sum}\psi_{j,l,t-s}
\]

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
P\left(X\right)=\underset{j\in[m]}{\sum}\left(\alpha_{j,t}\cdot\beta_{j,t}+\underset{l\in\left[k\right],\ s\in\left[|W_{l}|\right]}{\sum}\psi_{j,l,t-s}\right)
\]

\end_inset


\end_layout

\begin_layout Standard
where t here is set arbitrary from 
\begin_inset Formula $t\in[L]$
\end_inset

.
\end_layout

\begin_layout Standard
Second, the probability (13) of the base-state at a given position given
 the sequence X, denoted as 
\begin_inset Formula $\gamma$
\end_inset

 of size 
\begin_inset Formula $m\times L$
\end_inset

:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{align*}
\gamma_{j,t}= & P\left(y_{t}=(j,0)|X\right)=\frac{\alpha_{j,t}\cdot\beta_{j,t}}{P\left(X\right)}
\end{align*}

\end_inset

 
\end_layout

\begin_layout Standard
Third, the probability (14) of being at a base-state to base-state transition
 given the sequence X, denoted as 
\begin_inset Formula $\xi$
\end_inset

 of size 
\begin_inset Formula $m\times m\times L$
\end_inset

:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{align*}
\xi_{j_{1},j_{2},t}= & P\left(y_{t-1}=(j_{1},0),y_{t}=(j_{2},0)|X_{i}\right)=\frac{P\left(y_{t-1}=(j_{1},0),y_{t}=(j_{2},0),X\right)}{P\left(X\right)}=\\
= & \frac{\alpha_{j_{1},t-1}\cdot T_{j_{1},j_{2}}\cdot E_{j_{2},x_{t-o+1},...,x_{t}}\cdot\beta_{j_{2},t}}{P\left(X\right)}
\end{align*}

\end_inset


\begin_inset Newline newline
\end_inset

Finally, the probability (15) of being at a base-state to base-state transition
 given the sequence X, denoted as 
\begin_inset Formula $\xi$
\end_inset

 of size 
\begin_inset Formula $m\times k\times L$
\end_inset

:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{align*}
\eta_{j,l,t}= & P\left(y_{t-1}=(j,0),y_{t}=(j,l)|X\right)=\frac{\psi_{j,l,t}}{P\left(X\right)}
\end{align*}

\end_inset


\end_layout

\begin_layout Standard

\series bold
\bar under
M-step
\end_layout

\begin_layout Standard
With (13), (14), (15) at hand, we can calculate 
\begin_inset Formula $\theta_{max}$
\end_inset

 by assigning them at (9), (10), (11), (12).
 Namely:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
\pi_{j}=\gamma_{j,1}
\]

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
E_{j,b_{1},b_{2},...,b_{o}}=\frac{\sum_{t\in o,...,L}\gamma_{j,t}\cdot\boldsymbol{1}_{b_{1},...,b_{o}}(x_{t-o+1},...,x_{t})}{\sum_{t\in o,...,L}\gamma_{j,t}}
\]

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
T_{j_{1},j_{2}}=\frac{\underset{t\in2,...,L}{\sum}\xi_{j_{1},j_{2},t}}{\underset{t\in1,...,L-1}{\sum}\gamma_{j_{1},t}}
\]

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
G_{j,l}=\frac{\underset{t\in2,...,L}{\sum}\eta_{j,l,t}}{\underset{t\in1,...,L-1}{\sum}\gamma_{j_{1},t}}
\]

\end_inset

[multiple sequences learning]
\end_layout

\begin_layout Standard
For readability, the algorithm was described for learning a single sequence
 of observable variables.
 For learning multiple sequences at once, we will use the same method as
 introduced in the original paper of Baum-Welch algorithm (Rabiner, 1989),
 which is essentially summing each component during the method over all
 sequences whenever possible.
\end_layout

\begin_layout Standard

\series bold
\bar under
DATA
\end_layout

\begin_layout Standard
[Roadmap enhancers preprocessing]
\end_layout

\begin_layout Standard
[training on roadmap data]
\end_layout

\begin_layout Standard
[classification of regulation modules]
\end_layout

\begin_layout Standard
[choosing the PWMs]
\end_layout

\begin_layout Standard
[synthetic sequences genration]
\end_layout

\begin_layout Part*
Results
\end_layout

\begin_layout Subparagraph*
[tommy: prove and show know our limitation with a huge generated data-set,
 scatter plot of 
\backslash
theta values where it should go on the diagonal, real vs learned]
\end_layout

\begin_layout Subparagraph*
[tommy: position of the motifs]
\end_layout

\begin_layout Subparagraph*
[tommy: compare viterbi and max postirior]
\end_layout

\begin_layout Subparagraph*
[compare different hyperparameters.
 are we better than regular hmm? does the prelearning help? fixed base state
 E?]
\end_layout

\begin_layout Subparagraph*
[tommy: aligned heatmap sequences with the peak of the H3K27ac and post.
 prob., are they similar?]
\end_layout

\begin_layout Section*

\series bold
\bar under
Semi-Supervised Learning Scheme:
\end_layout

\begin_layout Enumerate

\series bold
Pre-training:
\series default
 Calculate maximal likelihood initialization parameters 
\begin_inset Formula $\theta_{0}$
\end_inset

 with from observed labeled dataset of sequences 
\begin_inset Formula $Y_{0}$
\end_inset

 and 
\begin_inset Formula $X_{0}$
\end_inset


\end_layout

\begin_layout Enumerate

\series bold
Unsupervised learning:
\series default
 Learn the 
\begin_inset Formula $\theta$
\end_inset

 given X unlabeled sequences by approximating 
\begin_inset Formula $\theta_{best}=$
\end_inset


\begin_inset Formula $argmax_{\theta}\mathcal{L}\left(\theta|X\right)$
\end_inset


\end_layout

\begin_layout Enumerate

\series bold
Predicting labels: 
\series default
Given learned 
\begin_inset Formula $\theta$
\end_inset

, infer hidden states Y for unlabeled X
\end_layout

\begin_layout Enumerate

\series bold
Tuning:
\series default
 Preform hyperparameters optimization
\end_layout

\begin_layout Standard
experiment graphs:
\end_layout

\begin_layout Standard

\series bold
per nucleotide binary classification (background vs enhancer)
\series default
 - heatmap (x-location relative to peak, y-sequence index, color-post.
 probability) use the 
\begin_inset Formula $\gamma_{i,*}$
\end_inset

 of enhancers where i is the state of the enhancer, pick only the 2000+-
 around the enhancer's peak, where the center is the max of the H3K27ac
 signal, showing the sequences are most likely enhancers surrounded by non
 enhancers .
 
\end_layout

\begin_layout Standard

\series bold
per sequence mutli-class classification (background vs enhancer)
\series default
 - heatmap (state number, y-sequence index, color-maximal post.
 probability for the sequence of that state) of 
\begin_inset Formula $\gamma$
\end_inset

 of enhancers, where the center is the max of the H3K27ac, showing the sequences
 are most likely enhancers surrounded by non enhancers 
\end_layout

\begin_layout Standard
per nucleotide classification 
\end_layout

\begin_layout Standard
per sequence classification
\end_layout

\begin_layout Standard
different hyperparameters k, o, m
\end_layout

\begin_layout Standard
different 
\end_layout

\begin_layout Standard
[per sequence binary classification - backgound vs enhancer]
\end_layout

\begin_layout Standard
[per nucleotide binary classification - backgound vs enhancer]
\end_layout

\begin_layout Standard
[per sequence binary classification - backgound vs enhancer]
\end_layout

\begin_layout Standard
[prediction on roadmap regulation modules]
\end_layout

\begin_layout Standard
[Whole genome classification?]
\end_layout

\begin_layout Standard
[Was HOP-HMM better?]
\begin_inset Newline newline
\end_inset


\end_layout

\begin_layout Standard

\series bold
\bar under
Possible Applications
\end_layout

\begin_layout Standard
labeled enhancer seqs from multiple motifs-> EM to learn E M F per floor
 + setting
\begin_inset Formula $T=\mathbb{I}_{m\times m}$
\end_inset

 -> posterior of whole genome with sliding window -> classify whole genome
\end_layout

\begin_layout Standard
learn E M F -> check correlation with TF expression
\end_layout

\begin_layout Standard
run EM on whole genome -> posterior of whole genome -> check correlation
 of posterior to ChIP-Seq of histone modifications
\end_layout

\begin_layout Standard
E M F T-> posterior of whole genome -> see if known critical SNPs are critical
 in classification
\end_layout

\end_body
\end_document
