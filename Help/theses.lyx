#LyX 2.2 created this file. For more info see http://www.lyx.org/
\lyxformat 508
\begin_document
\begin_header
\save_transient_properties true
\origin unavailable
\textclass article
\begin_preamble
\usepackage{babel}
\end_preamble
\use_default_options false
\maintain_unincluded_children false
\language english
\language_package default
\inputencoding auto
\fontencoding T1
\font_roman "default" "default"
\font_sans "default" "default"
\font_typewriter "default" "default"
\font_math "auto" "auto"
\font_default_family default
\use_non_tex_fonts false
\font_sc false
\font_osf false
\font_sf_scale 100 100
\font_tt_scale 100 100
\graphics default
\default_output_format default
\output_sync 0
\bibtex_command default
\index_command default
\paperfontsize default
\spacing single
\use_hyperref false
\papersize default
\use_geometry true
\use_package amsmath 2
\use_package amssymb 2
\use_package cancel 0
\use_package esint 1
\use_package mathdots 0
\use_package mathtools 0
\use_package mhchem 0
\use_package stackrel 0
\use_package stmaryrd 0
\use_package undertilde 0
\cite_engine natbib
\cite_engine_type numerical
\biblio_style plainnat
\use_bibtopic false
\use_indices false
\paperorientation portrait
\suppress_date false
\justification true
\use_refstyle 0
\index Index
\shortcut idx
\color #008000
\end_index
\leftmargin 2cm
\topmargin 2cm
\rightmargin 2cm
\bottommargin 2cm
\headheight 0cm
\headsep 0cm
\footskip 0cm
\secnumdepth 3
\tocdepth 3
\paragraph_separation skip
\defskip medskip
\quotes_language english
\papercolumns 1
\papersides 1
\paperpagestyle default
\tracking_changes false
\output_changes false
\html_math_output 0
\html_css_as_file 0
\html_be_strict false
\end_header

\begin_body

\begin_layout Title
High-Order and PWM Hidden Markov Model (HOP-HMM)
\end_layout

\begin_layout Section*
Abstract
\end_layout

\begin_layout Section*
Introduction
\end_layout

\begin_layout Standard
[TODO: rewrite this]
\end_layout

\begin_layout Standard
The TF inside the nucleus of specific tissues are thought to be a key factor
 in the activation of specific enhancer.
 The TFs form a transcription complex and are connected to the enhancer
 and promoter sequences on top of the TF binding sites (TFBS).
 Studies using TFBS of TF present in specific cell types are used to classify
 cell specific enhancer sequences.
 show heat map of AUC-ROC results.
 Between these TFBS, k-mer frequencies varies between enhancers and non
 regulatory 
\begin_inset Quotes eld
\end_inset

backgound
\begin_inset Quotes erd
\end_inset

 DNA, and was used to classify enhancers from backgound using only the k-mer
 distribution (Inbar and tomy, gkm-SVM), and is thought to play a role in
 spacial properties, nucleosome location and cleavage that cause accessibility
 of near-by TFBS.
 Using 44 out of 127 epigenetic data of Roadmap Project to select tissue
 specific enhancer sequences dataset.
 In our method, we look for different TFBS and k-mer presence in sequences
 to classify cell-specific sequences, inside regulatory modules.
\end_layout

\begin_layout Section*
Background
\end_layout

\begin_layout Standard
[description of enhancers, target genes and DNA folding]
\end_layout

\begin_layout Standard
Enhancers are non-coding regulatory DNA sequences that play a key role in
 the regulation of gene expression.
 In humans there are hundreds of thousands of enhancers, scattered in 98%
 of the non-coding regions of the genome, and they are usually between 100-1000
 bp long.
 When activated, the DNA folding draws the enhancer spatially closer to
 a regulatory element called promoter, resulting in the translation of a
 gene adjacent to the promoter.
 The enhancer's 
\begin_inset Quotes eld
\end_inset

target gene
\begin_inset Quotes erd
\end_inset

 (or target genes) is the expressed gene from this activation process.
 It can be located up to a megabase upstream or downstream from their activating
 enhancer (or enhancers), and are orientation independent to it.
 
\end_layout

\begin_layout Standard
[TADs]
\end_layout

\begin_layout Standard
Hi-C are techniques for measuring the spatial organization of chromatin
 in the nucleus.
 These procedures measure the number of occurrences were genome loci are
 in proximity.
 In recent years, Hi-C data was generated from multiple organisms (Dixon,
 J.
 R.
 et al.
 2012; Rao, S.
 S.
 P.
 et al.
 2014; Dixon, J.
 R.
 et al.
 2015; Sexton, T.
 et al.
 2012), and shown that the genome is partitioned into topologically associating
 domains (TADs), mostly under 10 mb long (F.
 Ramírez 2018).
 Within TADs sequence to sequence interaction are more frequent.
 TADs boundaries are enriched with DNA-biding protein motifs, and their
 location play a significant role in gene regulation.
 Gene-enhancer interactions occur when both are in the same TAD, and deletion
 of the motifs from TAD's boundaries may cause changes in the gene activation
 leading to developmental abnormalities in mouse embryos (Lupiáñez DG, et
 al.
 2015).
 It has been argued (S.
 Kadauke 2018) that the TADs are dynamic throughout the cell's life cycle,
 and with the change of the TADs structure the enhancer-gene activation
 couples may vary as well, suggesting the target gene is not predetermined
 by the the enhancer's sequence.
\end_layout

\begin_layout Standard
[TFs, cofactors and PIC]
\end_layout

\begin_layout Standard
TF usually bind to motifs on the DNA, which are short series of nucleotides,
 sometimes with gaps called TFBS.
 TFBSs are common in enhancer and promoter elements, and they are good predictor
s for enhancers location and the type of cell it will be active in.
\end_layout

\begin_layout Standard
Multiple studies have shown that genetic alternations in TFBS can affect
 the expression of its target gene and are a major cause of human diseases
 (reviewed in I.
 Miguel-Escalada 2015) [examples: (Soldner F.
 2016, Smemo S.
 2012, Benko S.
 2009, E.
 S.
 Emison, 2005, Lettice L.A 2003].
 Folding of DNA allows the enhancer-promoter interactions, and recruiting
 of other cofactor proteins to the already bounded TFs.
 Some of the recruited cofactors cause nearby chromatin modification and
 transcription activation.
 The TFs and cofactors form a transcription preinitiation complex (PIC)
 which is a very large assembly of proteins (more than one hundred in humans)
 that recruit the RNA Polymerase (RNA pol II) to invokes the transcription
 process of the adjacent gene: it opens the double stranded DNA, so that
 one strand of nucleotides is exposed and becomes a template for RNA synthesis.
 
\begin_inset Newline newline
\end_inset


\begin_inset Newline newline
\end_inset

[enhancer status, chromatine methylation and accessibility] 
\end_layout

\begin_layout Standard
Cells of different types and in different operation modes differ by gene
 expression patterns that are regulated by enhancers and promoters activity
 patterns.
 The activity status of enhancers is connected to multiple factors that
 are detectable on a genome scale.
 
\begin_inset Newline newline
\end_inset


\begin_inset Newline newline
\end_inset

Chromatin modifications signatures, or 
\begin_inset Quotes eld
\end_inset

histone marks
\begin_inset Quotes erd
\end_inset

 are predictive of enhancer position and activity status and can be assessed
 by chromatin immunoprecipitation followed by deep sequencing (ChIP–seq)
 (Visel et al.
 2009; Firbi et al.
 2010; Fernandez et al.
 2012) .
 H3k4me1 and H3k27ac are among the predominant histone marks of active enhancers
, where H3k4me1 are enriched on transcribed genes and 
\begin_inset Quotes eld
\end_inset

primed
\begin_inset Quotes erd
\end_inset

 enhancers prior activation (calo et al 2013), and is thought to precede
 the H3k27ac modification (Creyghton et al., 2010; Rada-Iglesias et al., 2011;
 Zentner et al., 2011) which is known to occur during the activation.
 Other histone marks that are present on active enhancers and are used for
 their detection are H3k9ac (Ernst et al., 2011; Karmodiya et al., 2012; Krebs
 et al., 2011; Zentner et al., 2011) and H3K18ac (Jin et al., 2011).
 
\begin_inset Newline newline
\end_inset

Even though H3k27ac have been identified as an important mark for distinguishing
 active enhancers from poised enhancers (Creyghton et al.
 2010), it is not enough as its own since when present alongside H3k4me3
 it is an indication for active promoters [Heintzman et al., 2007].
 In contrast, H3k27ac absence and H3k4me1and H3k27me3 enrichment are typical
 for poised enhancers (Creyghton et al, 2010).
\begin_inset Newline newline
\end_inset


\begin_inset Newline newline
\end_inset

DNA methylation at 5-methycytosine has been involved in genome silencing
 in multiple processes (Jones, 2012), and has been documented as largely
 correlated with gene expression inhibition when present in promoters.
 In enhancer elements, anticorrelation was found between DNA methylation
 density and enrichment of active enhancer histone marks (Koch and Andrau,
 2011; Schmidl et al., 2009; Stadler et al., 2011; Thurman et al., 2012) and
 TF binding (Neph et al., 2012; Stadler et al., 2011; Thurman et al., 2012),
 although the cause and consequence relationship underlying these correlations
 is not yet clear.
 Since the scenario of TF binding on an enhancer requires an accessible
 DNA region, DNase I hypersensative sites are used for detecting a potential
 DNA cleavages that have the potential of being regulatory elements, in
 usually a better resolution than histone marks.
\end_layout

\begin_layout Standard

\bar under
[ inter TFBS sequence, Conservation]
\end_layout

\begin_layout Standard
Conserved non-coding elements (CNE) reside in clusters, usually with low
 gene density but with vicinity to genes.
 Typically, CNE are structured in arrays called GRB, with a mean length
 of 1.4 Mb (X.
 Dong et al.
 2009).
 The correlation between conservation of non-coding region and enhancer
 functionality is not strong.
 Some verified enhancers are weakly or not conserved between distant species
 (M.
 Friedli et al.2010; J.M.
 Rosin et al.
 2013; L.
 Taher 2011; D Boffelli, 2004, K.
 Lindblad-Toh 2011) and some highly conserved areas in the mouse genome
 are not associated to regulatory activity and their deletion and yielded
 viable mice (N.
 Ahituv et al.
 2007).
 Nevertheless, an assay of elements with 100% sequence identity of over
 200 bp between human and mouse found that 50% showed enhancers activity
 in mice (Visel et al., 2007).
 The reason for such ultra-conservation of 200 bp sequences when the TFBS
 is only 4-8 bp long is unclear.
 It is possible that these conserved sequences are actually long assembly
 of overlapping TFBS or that the enhancer has another function as a eRNA,
 that the exact nature of its mechanisms is no understood (M.
 Haeussler 2011, Andersson et al.
 2014).
 
\end_layout

\begin_layout Standard
TODO: add a word on conservation of TFBS
\end_layout

\begin_layout Standard
[problems with epigenetic data to identify enhancers]
\end_layout

\begin_layout Standard
The currently most accurate method for predicting the location of tissue
 specific enhancers, is analyzing the histone marks and TF and cofactors
 presence using ChIP-seq from a cell line or from a tissue, combined with
 DNase I hypersensative (DHS).
 The main disadvantage of this method is this process is inherently limited
 to the tissues we can extract and isolate for the epi-genetic examination.
 Another disadvantage of this method is the need for live cells for the
 verification of the regulatory activity of a sequence.
 The persecute for an efficient computational method for predicting the
 functional nature of sequences 
\begin_inset Quotes eld
\end_inset

in-silico
\begin_inset Quotes erd
\end_inset

 has produced positive, yet far from sufficient results in the last years,
 as reviewed in (D.
 Kleftogiannis et al.
 2016).
\end_layout

\begin_layout Standard
[machine learning]
\end_layout

\begin_layout Standard
There are several achievements in the task of predicting epi-genetic properties
 of DNA elements given only their sequence using machine learning algorithms.
 DeepSEA predicts chromatin modifications given a sequence, from which a
 regulatory activity could be deduced, and Basset and deltaSVM predicts
 accessibility.
 gkm-SVM (Beer et al.
 2014) uses gapped kmers as features for an SVM classifier to predict the
 role of DNA sequences.
 The disadvantage of these method is their need for a training data of known
 regulatory elements, which are known mainly from GWAS surveys done on 127
 obtained human cell types in the Roadmap and ENCODE projects (Kundaje et
 al.
 2015; Ernst et al.
 2011).
 The number of different cell types in the human body is estimated to be
 higher than 2200 (Hatano et al.
 2011, Diehl et al.
 2016), where then number and location of tissue specific enhancers of the
 rest of the cell types is a mystery.
\end_layout

\begin_layout Standard
[HMM background]
\end_layout

\begin_layout Standard
Multiple signal processing algorithms have been used in computational biology,
 and HMM is especially popular among them.
 Hidden Markov model (HMM) is a statistical model proposed by Leonard Baum
 (Baum et al.
 1966) and is based on the Markov model for modeling regions with alternating
 frequencies of patterns and symbols.It was used in various engineering fields
 since the 1980s especially in speech recognition, character recognition
 and digital communication and was adopted in the computational biology
 field.
 The reason HMM is effective in the task of DNA classification is the nucleotide
 frequencies differences between different types of DNA elements.
 The Baum-Welch and Viterbi algorithms can detect elements with distinguishable
 letters distributions without having to train on a labeled dataset.
 The disadvantage of the model is the lack of consideration for TFBSs or
 multi-necleotides motifs frequency differences between DNA segments.
\end_layout

\begin_layout Standard
[PWMs and motif to classify tissue specific enhancers]
\end_layout

\begin_layout Standard
[k-mer to classify tissue specific enhancers]
\end_layout

\begin_layout Standard
[HMM to classify tissue specific enhancers]
\end_layout

\begin_layout Standard
[other machine learning work to classify tissue specific enhancers]
\end_layout

\begin_layout Standard
[Why the HOP-HMM approach to the problem differently]
\end_layout

\begin_layout Standard
[other stories]
\end_layout

\begin_layout Standard
There are other mechanisms and non classic scenarios that involve enhacners:
 are intergenic enhancers, very long enhancers
\end_layout

\begin_layout Standard
TODO: k-mer frequencies varies between enhancers and non regulatory 
\begin_inset Quotes eld
\end_inset

background
\begin_inset Quotes erd
\end_inset

 DNA, and was used to classify enhancers from backgound using only the k-mer
 distribution (Inbar and Tommy), 
\end_layout

\begin_layout Standard
TODO: [Levin, VISTA] experiments shows that insertion of a sequence without
 any epigenetic information will activate an enhancer with a near by blue
 coloring gene.
 This implies that for the newly introduced sequences to operate as an enhancer
 and target gene, all that is needed is its sequence in arbitrary location
 without additional epigenetic information.
 
\end_layout

\begin_layout Standard
From (C.
 Bessière et al 2018, 
\begin_inset Quotes eld
\end_inset

Probing instructions for expression regulation in gene nucleotide compositions
\begin_inset Quotes erd
\end_inset

):
\end_layout

\begin_layout Standard
\begin_inset Quotes eld
\end_inset

Several approaches have tackled this problem by modeling gene expression
 based on epigenetic marks, with the ultimate goal of identifying driving
 regions and associated genomic variations that are clinically relevant
 in particular in precision medicine.
 However, these models rely on experimental data, which are limited to specific
 samples (even often to cell lines) and cannot be generated for all regulators
 and all patients.
 In addition, we show here that, although these approaches are accurate
 in predicting gene expression, inference of TF combinations from this type
 of models is not straightforward.
 Furthermore these methods are not designed to capture regulation instructions
 present at the sequence level, before the binding of regulators or the
 opening of the chromatin.
\begin_inset Quotes erd
\end_inset


\end_layout

\begin_layout Section*
Methods
\end_layout

\begin_layout Standard
[sequence classification, generative models, HMM]
\end_layout

\begin_layout Standard
Markov model (A.A.
 Markov, 1906) is a stochastic model named after Andrey Markov a Russian
 mathematician.
 In a Markov model, at any time the system is at one of m states 
\begin_inset Formula $\left\{ S_{1},...,S_{m}\right\} $
\end_inset

, where the first state is sampled from a distribution 
\begin_inset Formula $\pi_{i}=P\left(y_{0}=S_{i}\right)$
\end_inset

 and the probability of transitions between the states is denoted by T 
\begin_inset Formula $T_{i,j}=P\left(y_{t}=S_{i}|y_{t-1}=S_{j}\right)$
\end_inset

.
 The system's travel over the states is called a Markov process, and the
 sequence of states visited in the process is called a Makov chain.
\begin_inset Newline newline
\end_inset


\begin_inset Newline newline
\end_inset


\begin_inset Graphics
	filename Figures/Markov_state.png
	scale 40

\end_inset


\end_layout

\begin_layout Standard

\series bold
\emph on
Figure: Markov model with 4 states, marked as 
\begin_inset Formula $S_{1},S_{2},S_{3},S_{4}$
\end_inset

 with two of all possible transition probabilities written.
 T matrix describes the transition distributions, where 
\begin_inset Formula $T_{i,*}$
\end_inset

 is the distribution of the system's next step when in 
\begin_inset Formula $S_{i}$
\end_inset

.
\end_layout

\begin_layout Standard
Hidden Markov model (HMM) is a system that travels over hidden states in
 a Markov process, and while doing so it emits variables called observed
 variables.
 In this generation process, a single observed variable is emitted per system's
 step, and so the observed sequence is generated with the same length as
 the hidden Markov chain.
 The observed variables 
\begin_inset Formula $V_{1},...,V_{n}$
\end_inset

 are sampled from an emission distribution E 
\begin_inset Formula $E_{i,j}=P(x_{t}=V_{j}|y_{t}=S_{i})$
\end_inset

, that is conditioned on the system's hidden state.
 Similarly to the Markov model, the distribution to the first hidden state
 is marked as 
\begin_inset Formula $\pi$
\end_inset

 and the transition distribution is marked as 
\begin_inset Formula $T$
\end_inset

.
\end_layout

\begin_layout Standard
For example, assuming the DNA are composed of genes enhancers and background
 regions, with each having different nucleotide frequency, then we can say
 that the DNA sequence was generated by a HMM with underlying sequence of
 4 hidden states: gene, promoter, enhancer and background where each has
 its own nucleotide frequency.
 The emitted observed DNA sequence X is determined by the underlying hidden
 sequence Y that describes the 
\begin_inset Quotes eld
\end_inset

mode
\begin_inset Quotes erd
\end_inset

 of the sequence in each position.
\begin_inset Newline newline
\end_inset


\end_layout

\begin_layout Standard
\begin_inset Graphics
	filename Figures/HMM.jpg
	scale 60

\end_inset


\end_layout

\begin_layout Standard

\series bold
\emph on
Fig: a HMM chain with length of 5.
 The underlying hidden states (up) are generated in a Mrokov process and
 each state emits an observed variable, sampled from the state's emission
 probability.
\end_layout

\begin_layout Standard

\series bold
\bar under
Goals
\end_layout

\begin_layout Standard
As a generative model, HMM relies on the assumption that the observed DNA
 sequence 
\begin_inset Formula $\overrightarrow{X}^{N}=x_{1},...,x_{N}$
\end_inset

 was generated by a parameterized model 
\begin_inset Formula $\theta$
\end_inset

, and has an hidden state sequence 
\begin_inset Formula $\overrightarrow{Y}^{N}=y_{1},...,y_{N}$
\end_inset

 that was generated alongside it.
 Given an observed sequence 
\begin_inset Formula $\overrightarrow{X}^{N}$
\end_inset

, our three goals here are:
\end_layout

\begin_layout Standard
1.
 Finding model parameters 
\begin_inset Formula $\theta$
\end_inset

 that fits the most to the observed sequence.
 Formally, finding the 
\begin_inset Formula $\theta$
\end_inset

 that maximizes the likelihood of the observed sequences
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
argmax_{\theta}\mathcal{L}\left(\theta|\overrightarrow{X}^{N}\right)=argmax_{\theta}\sum_{i\in[N]}log\left(P_{\theta}(x_{i})\right)
\]

\end_inset


\end_layout

\begin_layout Standard
2.
 Using the maximum likelihood 
\begin_inset Formula $\theta$
\end_inset

, we would like to find the posterior probability of each individual hidden
 state, 
\begin_inset Formula $P_{\theta}\left(y_{t}|X\right)$
\end_inset


\end_layout

\begin_layout Standard
3.
 Using the maximum likelihood 
\begin_inset Formula $\theta$
\end_inset

, we would like to find the most probable hidden sequence Y, 
\begin_inset Formula $argmax_{\overrightarrow{Y}^{N}}\mathcal{L}\left(\overrightarrow{Y}^{N}|\overrightarrow{X}^{N};\theta\right)$
\end_inset

 
\end_layout

\begin_layout Standard
We will see that goal 1 and 2 can be achieved by using the Baum-Welch algorithm,
 named after Leonard E.
 Baum and Lloyd R.
 Welch who developed it and the HMM it solves during the 1960s and 1970s.
 It applies the well known EM algorithm to that infers the maximize the
 likelihood HMM parameters and then the posterior probability, iteratively.
 Goal 3 could then be achieved by using the Viterbi algorithm on the posterior
 probability, named after Andrew Viterbi who proposed it in 1967.
\end_layout

\begin_layout Subsection*

\bar under
Baum-Welch Algorithm, EM for HMM
\end_layout

\begin_layout Standard
In the maximal-likelihood estimation problem before us, we have the observed
 
\begin_inset Formula $X$
\end_inset

 and we would like find the parameters that maximize the likelihood of it,
 
\begin_inset Formula $\theta^{*}=argmax_{\theta}L\left(\theta|X\right)$
\end_inset

.
 This likelihood function 
\begin_inset Formula $L\left(\theta|X\right)$
\end_inset

, is called the incomplete-data likelihood function, is not enough for our
 task of it's optimization, hence we must assume a the existence of hidden
 variables Y.
 The strategy of the EM algorithm is to optimize the expected value of the
 complete-data log-likelihood 
\begin_inset Formula $P\left(X,Y|\theta\right)$
\end_inset

 while assuming 
\end_layout

\begin_layout Standard

\end_layout

\begin_layout Standard
fixed observed X (the DNA sequence) and 
\begin_inset Formula $\theta_{t}$
\end_inset

 (the model's parameters from previous iteration).
 This likelihood function is in fact a random variable since 
\begin_inset Formula $Y$
\end_inset

 is a random unknown variables that were sampled by a model with parameters
 
\begin_inset Formula $\theta$
\end_inset

.
 We can 
\end_layout

\begin_layout Standard
we define: 
\begin_inset Formula 
\[
Q\left(\theta,\theta_{t}\right)=E_{Y}\left[log\left(P\left(X,Y|\theta\right)\right)|X,\theta_{t}\right]=\sum_{Y\in\left[m\right]^{N}}log\left(P\left(X,Y|\theta\right)\right)P\left(X,Y|\theta_{t}\right)
\]

\end_inset

 where:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
P(X,Y|\theta)=\pi_{y_{0}}\prod_{t=1}^{N}T_{y_{t-1},y_{t}}E_{y_{t},x_{t}}
\]

\end_inset


\end_layout

\begin_layout Standard

\series bold
\bar under
E step
\series default
\bar default
,
\end_layout

\begin_layout Standard
################################################################
\end_layout

\begin_layout Standard
http://imaging.mrc-cbu.cam.ac.uk/methods/BayesianStuff?action=AttachFile&do=get&targ
et=bilmes-em-algorithm.pdf
\end_layout

\begin_layout Standard
Page 11 and page 2
\end_layout

\begin_layout Standard
################################################################
\end_layout

\begin_layout Standard
The EM algorithm 
\end_layout

\begin_layout Standard
an EM iterative algorithm where in each iteration t the probability of being
 in a hidden state at position i is calculated 
\begin_inset Newline newline
\end_inset


\begin_inset Formula 
\[
\gamma_{t}=P_{\theta_{t}}(y_{i}|x_{1:n})
\]

\end_inset

 
\end_layout

\begin_layout Standard
using the forward and backward dynamic algorithms, and them the maximum
 likelihood 
\begin_inset Formula $\theta_{t+1}$
\end_inset

 is calculated using this probabilities 
\begin_inset Formula 
\[
\theta_{t+1}=argmax_{\theta}(\gamma_{t}|\theta)
\]

\end_inset

 
\end_layout

\begin_layout Standard
[Generalized Hidden Markov Model (GHMM)]
\end_layout

\begin_layout Standard
HOP-HMM models the structure of enhancers containing TFBSs inside them.
 In this model, there are two types of states: PWM sub state, and base states,
 where each type of enhancer has its own base state, and the base can transfer
 into one of it's PWM sub states.
 Each PWM sub state emits a single motif sampled from its PWM, which is
 fixed and is given to the model.
 The base states emit a single letter each time, where the emission is condition
al to the previous k letters emitted.
 This method is useful to express both the lack of frequent long motifs
 in the inter-TFBS parts of the enhancer, and the motifs that are not in
 the fixed given set of PWMs of the HOP-HMM model.
\end_layout

\begin_layout Standard
A model that contains states that emit motifs sampled from PWMs was previously
 demonstrated (as described in [T.
 Kaplan et al 2011]) and as such it is considered a generalized hidden Markov
 model (gHMM) is a variant of HMM in which states may emit multiple letters.other
 states that emit letters depending on previous letters in the observed
 sequence.
 Similarly to HMM, it assumes an underlying hidden sequence is present that
 are sampled from a Markov chain.
 
\end_layout

\begin_layout Standard
HOP-HMM is a high-order emission base-states and PWM emission sub-states
 HMM from a dataset of N observations sequences 
\begin_inset Formula $\mathcal{X}=\left(X_{1},...,X_{N}\right)$
\end_inset

 where each observation sequence is L nucleotides long 
\begin_inset Formula $X_{i}=\left(x_{1}^{i},...,x_{L}^{i}\right)$
\end_inset

.
 We assume an underlying hidden variable sequences 
\begin_inset Formula $\mathcal{Y}=\left(Y_{1},...,Y_{N}\right)$
\end_inset

 where each underlying sequence is also L variables long 
\begin_inset Formula $Y_{i}=y_{1}^{i},...,y_{L}^{i}$
\end_inset

.
 Let the space of underlying states be 
\begin_inset Formula $\Upsilon=\left\{ 1,2,...,m\right\} \times\left\{ 0,1,...,k\right\} $
\end_inset

 
\begin_inset Newline newline
\end_inset


\end_layout

\begin_layout Standard
[sequences that could have been generated by HOP-HMM]
\end_layout

\begin_layout Standard
TOMMY:
\end_layout

\begin_layout Standard
quite abstract and hard to follow.
 try to be more concrete.
 See above.
 Add Figure!
\end_layout

\begin_layout Standard
not clear enough.
 I think your best option is to separate the figure into two parts, like
 I said before.
 Begin by a single "layer" and first describe the automaton and the transition
 probabilities (maybe with a matrix).
 Then show the generated sequence with the (hidden) states above.
 You can also plot the dependencies among them (Markovian model).
 Then make the model more complex, and keeping explaining with automaton
 figures.
\end_layout

\begin_layout Standard

\series bold
\bar under
Emission and Transition
\end_layout

\begin_layout Standard
Underlying states emit the observed sequence are of two types: base-states
 and their sub-states.
 We mark the j'th base-state as 
\begin_inset Formula $(j,0)$
\end_inset

 for 
\begin_inset Formula $j\in\{1,...,m\}$
\end_inset

 and its l'th sub-state as 
\begin_inset Formula $(j,l)$
\end_inset

 for 
\begin_inset Formula $l\in\{1,...,k\}$
\end_inset

.
 Denote the base-state emission order by o, meaning a base-state emits a
 letter sampled from an emission matrix 
\series bold

\begin_inset Formula $E$
\end_inset

 
\series default
that depends on previous 
\begin_inset Formula $o-1$
\end_inset

 letters.
\end_layout

\begin_layout Standard
Sub-state emits multiple letters sampled from a PWM that is fixed and isn't
 learned in the training.
 Denote
\begin_inset Formula $W_{l}$
\end_inset

 the PWM of the l'th sub-states, which is shared between the l'th sub-states
 of all base-states.
 
\begin_inset Newline newline
\end_inset

 
\begin_inset Newline newline
\end_inset

 
\begin_inset Graphics
	filename Figures/Slide4.eps
	scale 40

\end_inset


\begin_inset Newline newline
\end_inset

 
\series bold
\shape italic
Figure 4: emission and transition process between base-state.
 The upper flow represent transition from base-state to the same base-state,
 through a sub-state that emits a motif.
 The lower flow represent transition between two base-state using the T
 transition matrix, similar to the conventional HMM.
\series default
\shape default

\begin_inset Newline newline
\end_inset

 
\begin_inset Newline newline
\end_inset


\end_layout

\begin_layout Standard
After emitting a single letter, the j'th base-state has a probability 
\begin_inset Formula $F_{j}$
\end_inset

 to make a transition into one of its sub-state and emit a motif and probability
 
\begin_inset Formula $1-F_{j}$
\end_inset

 to make a transition into one of the base-states and emit a single letter.
 The distribution of transitions between base-states is set by T matrix,
 and between base-state to its sub-states by G matrix.
 After emitting a motif in a sub-state, the next state will be the sub-state's
 base-state where it will emit a single letter.
\begin_inset Newline newline
\end_inset


\end_layout

\begin_layout Standard
\begin_inset Graphics
	filename Figures/Slide1.eps
	scale 50
	clip

\end_inset


\end_layout

\begin_layout Standard

\series bold
\shape italic
Figure 1: The hidden variable states graph of the HOP-HMM.
 The left hexagons represent base-states, and the circles in the right part
 of each row's represent its sub-states.
 
\series default
\shape default

\begin_inset Newline newline
\end_inset


\end_layout

\begin_layout Standard
\begin_inset Graphics
	filename Figures/Slide2.JPG
	scale 50
	clip

\end_inset


\end_layout

\begin_layout Standard

\series bold
\shape italic
Figure 2: high-order emission of base-states.
 Each emission is dependent on the hidden base-state and o-1 previous observatio
ns.
\series default
\shape default

\begin_inset Newline newline
\end_inset


\end_layout

\begin_layout Standard
\begin_inset Graphics
	filename Figures/Slide3.JPG
	scale 50

\end_inset


\end_layout

\begin_layout Standard

\series bold
\shape italic
Figure 3: PWM emission of sub-states.
\series default
\shape default

\begin_inset Newline newline
\end_inset

 
\begin_inset Newline newline
\end_inset


\end_layout

\begin_layout Standard

\series bold
\bar under
Parameters
\end_layout

\begin_layout Standard
An HOP-HMM 
\begin_inset Formula $\theta=\{\pi,E,T,G,F\}$
\end_inset

 is parameterized by: 
\end_layout

\begin_layout Itemize
\begin_inset Formula $\pi:\text{ }m\times1$
\end_inset

 initial base-state distribution vector 
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
\pi_{j}=P(y_{1}=j)
\]

\end_inset


\end_layout

\begin_layout Itemize
\begin_inset Formula $E:\text{ }m\times\underset{o\ times}{\underbrace{4\times4\times...\times4}}$
\end_inset

 the base-state high-order emission probability matrix 
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
E_{j,x_{t-o+1},x_{t-o+2},...,x_{t}}=P\left(x_{t}|y_{t}=(j,0),x_{t-o+1},...,x_{t-1}\right)
\]

\end_inset


\end_layout

\begin_layout Itemize
\begin_inset Formula $T:\text{ }m\times m$
\end_inset

 the transition probability matrix 
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
T_{j_{1},j_{2}}=P\left(y_{t+1}=(j_{2},0)|y_{t}=(j_{1},0)\right)
\]

\end_inset


\end_layout

\begin_layout Itemize
\begin_inset Formula $G:\text{ }m\times k$
\end_inset

 the sub-state entry probability matrix 
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
G_{j,l}=P\left(y_{t+1:t+|W_{l}|}=(j,l)|y_{t}=(j,0)\right)
\]

\end_inset


\end_layout

\begin_layout Part*
HOP-EM Algorithm
\end_layout

\begin_layout Standard
(TODO: where should I put this?)
\end_layout

\begin_layout Standard
Denote 
\begin_inset Formula $L_{M}(\overline{x})$
\end_inset

 as the likelihood of motif 
\begin_inset Formula $\overline{x}$
\end_inset

 , i.e.
 the probability that 
\begin_inset Formula $\overline{x}$
\end_inset

 was generate by PWM M
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
L_{M}(\overline{x})=P(\overline{x}|M)=\underset{i\in\{1,...,|\overline{x}|\}}{\prod}M_{\overline{x}_{i},i,}
\]

\end_inset


\end_layout

\begin_layout Standard
HMM problem:
\end_layout

\begin_layout Standard
Given the observation sequence X, and a model 
\begin_inset Formula $\theta=(\pi,E,T,G)$
\end_inset


\end_layout

\begin_layout Standard
How can we compute the probability of the hidden states at each position
 
\begin_inset Formula $P(y_{t}|X,\theta)$
\end_inset

?
\end_layout

\begin_layout Standard
How can we calculate 
\begin_inset Formula $P(X|\theta)$
\end_inset

, and what model 
\begin_inset Formula $\theta'$
\end_inset

 maximizes the 
\begin_inset Formula $P(X|\theta)$
\end_inset

?
\end_layout

\begin_layout Section*

\bar under
E-Step:
\bar default
 Forward Algorithm
\end_layout

\begin_layout Standard
As in the Forward Algorithm (Rabiner, 1989) we calculate the distribution
 of the t'th hidden-state, considering the observations from the beginning
 of the sequence until the t'th letter.
 In this version, it is enough to calculate only the probability of being
 in the base-states and not the sub-states, i.e.
 
\begin_inset Formula $\alpha_{j,t}=P\left(y_{t}=(j,0),x_{1:t}\right)$
\end_inset

.
 We calculate 
\begin_inset Formula $\alpha_{j,t}$
\end_inset

 iterating over 
\begin_inset Formula $t=1,2,...,L$
\end_inset

 where the initial for 
\begin_inset Formula $t=1$
\end_inset

: 
\begin_inset Formula $\alpha_{j,1}=P\left(y_{1}=(j,0),x_{1}\right)=\pi_{j}\cdot\underset{i_{1},...,i_{o-1}}{\sum}E_{j,i_{1},...,i_{o-1},x_{1}}$
\end_inset


\end_layout

\begin_layout Standard
For 
\begin_inset Formula $t\in\{1...L\}$
\end_inset

 the table is filled dynamically:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{align*}
\alpha_{j,t}= & P(y_{t}=(j,0),x_{1:t})=\\
= & \underset{\text{base-state transitions}}{\underbrace{\sum_{j'\in\{1,...,m\}}\alpha_{j',t-1}\cdot T_{j',j}\cdot E_{j,x_{t-o+1}^{i},...,x_{t}^{i}}}}\\
+ & \underset{\text{sub-state transitions}}{\underbrace{\sum_{l\in\{1,...,k\}}\alpha_{j,t-|W_{l}|-1}\cdot G_{j,l}\cdot L_{W_{l}}\left(x_{t-|W_{l}|},...,x_{t-1}\right)\cdot E_{j,x_{t-o+1},...,x_{t}}}}
\end{align*}

\end_inset


\end_layout

\begin_layout Standard
Explanation:
\end_layout

\begin_layout Standard
From the law of total probability, the probability 
\begin_inset Formula $\alpha_{j,t}$
\end_inset

 is the sum of probabilities of all the possible transition that ended in
 the base-state (j,0).
 The possible transitions are base-state to base-state transitions and sub-state
 to base-state transitions.
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
\alpha_{j,t}=P\left(y_{t}=(j,0),x_{1:t}\right)=
\]

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
=\underset{\text{base-state transitions}}{\underbrace{\underset{j'\in\{1,...,m\}}{\sum}P\left(y_{t-1}=(j',0),y_{t}=(j,0),x_{1:t}\right)}}+\underset{\text{sub-state transitions}}{\underbrace{\underset{l\in\{1,...,k\}}{\sum}P\left(y_{t-|W_{l}|:t-1}=(j,l),y_{t-|W_{l}|-1}=(j,0),x_{1:t}\right)}}
\]

\end_inset


\end_layout

\begin_layout Standard
Develop of a sub-state transition using the chain rule:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{align*}
P\left(y_{t-|W_{l}|:t-1}=(j',l),y_{t-|W_{l}|-1}=(j,0),x_{1:t}\right)= & \,P\left(y_{t-|W_{l}|-1}=(j,0),x_{1:t-|W_{l}|-1}\right)\cdot\\
 & \cdot P\left(y_{t-|W_{l}|:t-1}=(j,l)|y_{t-|W_{l}|-1}=(j,0),x_{1:t-|W_{l}|-1}\right)\\
 & \cdot P\left(x_{t-|W_{l}|:t-1}|y_{t-|W_{l}|:t-1}=(j,l),y_{t-|W_{l}|-1}=(j,0),x_{1:t-|W_{l}|-1}\right)\\
 & \cdot P\left(x_{t}|y_{t}=(j,0),x_{1:t-1},y_{t-|W_{l}|:t-1}=(j,l),y_{t-|W_{l}|-1}=(j,0)\right)
\end{align*}

\end_inset


\end_layout

\begin_layout Standard
Because of 
\begin_inset Formula $x_{t}$
\end_inset

 is dependent on only 
\begin_inset Formula $y_{t}$
\end_inset

 (and also 
\begin_inset Formula $x_{t-o:t-1}$
\end_inset

 if 
\begin_inset Formula $y_{t}$
\end_inset

 is a base-state) and 
\begin_inset Formula $y_{t}$
\end_inset

 is dependent on only 
\begin_inset Formula $y_{t-1}$
\end_inset

, we can simplify the probabilities:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{align*}
P\left(y_{t-|W_{l}|:t-1}=(j',l),y_{t-|W_{l}|-1}=(j,0),x_{1:t}\right)= & \,P\left(y_{t-|W_{l}|-1}=(j,0),x_{1:t-|W_{l}|-1}\right)\\
 & \cdot P\left(y_{t-|W_{l}|:t-1}=(j,l)|y_{t-|W_{l}|-1}=(j,0)\right)\\
 & \cdot P\left(x_{t-|W_{l}|:t-1}|y_{t-|W_{l}|:t-1}=(j,l)\right)\\
 & \cdot P\left(x_{t}|y_{t}=(j,0),x_{t-o:t-1}\right)
\end{align*}

\end_inset


\end_layout

\begin_layout Standard
We can now replace the received terms with the components of 
\begin_inset Formula $\theta$
\end_inset

 and with already filled 
\begin_inset Formula $\alpha$
\end_inset

 cells:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
P\left(y_{t-|W_{l}|:t-1}^{i}=(j,l),y_{t-|W_{l}|-1}^{i}=(j,0),x_{1:t}^{i}\right)=\alpha_{j,t-|W_{l}|-1}\cdot G_{j,l}\cdot L_{W_{l}}\left(x_{t-|W_{l}|},...,x_{t-1}\right)\cdot E_{j,x_{t-o+1},...,x_{t}}
\]

\end_inset


\begin_inset Newline newline
\end_inset


\end_layout

\begin_layout Standard
This process is similar to the base-state transition.
 Using the chain rule:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
P\left(y_{t-1}=(j',0),y_{t}=(j,0),x_{1:t}\right)=P\left(y_{t-1}=(j',0),x_{1:t-1}\right)\cdot P\left(y_{t}=(j,0)|y_{t-1}=(j',0),x_{1:t-1}\right)\cdot P\left(x_{t}|y_{t}=(j,0),y_{t-1}=(j',0),x_{1:t-1}\right)=
\]

\end_inset


\end_layout

\begin_layout Standard
Using the conditional independencies to simplify the probabilities:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
=P(y_{t-1}=(j',0),x_{1:t-1})\cdot P\left(y_{t}=(j,0)|y_{t-1}=(j',0)\right)\cdot P\left(x_{t}|y_{t}=(j,0),x_{1:t-1}\right)=\alpha_{j',t-1}\cdot T_{j',j}\cdot E_{j,x_{t-o+1},...,x_{t}}
\]

\end_inset


\end_layout

\begin_layout Standard

\series bold
\bar under
Backward Algorithm
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{align*}
\beta_{j,t}= & P\left(x_{t+1:L}|y_{t}=(j,0)\right)=\\
= & \underset{\text{base-state transitions}}{\underbrace{\sum_{j'\in\{1,...,m\}}\beta_{j',t+1}\cdot E_{u,x_{t-o+2},...,x_{t+1}}\cdot T_{j,j'}}}\\
+ & \underset{\text{sub-state transitions}}{\underbrace{\sum_{l\in\{1,...,k\}}\beta_{j,t+|W_{l}|+1}\cdot L_{W_{l}}\left(x_{t+1},...,x_{t+|W_{l}|}\right)\cdot E_{j,x_{t-o+|W_{l}|+2},...,x_{t+|W_{l}|+1}}\cdot G_{j,l}}}
\end{align*}

\end_inset


\end_layout

\begin_layout Standard
TODO: how out of range is handled for the PWMs.
 The problem of peaking before the t when doing a high-order emission of
 the base-states
\end_layout

\begin_layout Standard
Law of total probabilty:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
P\left(x_{t+1:L}|y_{t}=(j,0)\right)=
\]

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
=\underset{\text{base-state transition}}{\underbrace{\sum_{j'}P\left(y_{t+1}=(j',0),x_{t+1:L}|y_{t}=(j,0)\right)}}+\underset{\text{sub-state transition}}{\underbrace{\sum_{l}P\left(y_{t+1:t+\left|W_{l}\right|}=(j,l),y_{t+\left|W_{l}\right|+1}=(j,0),x_{t+1:L}|y_{t}=(j,0)\right)}}
\]

\end_inset


\end_layout

\begin_layout Standard
For the base-state transition term,using the chain rule:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{align*}
P\left(y_{t+1}=(j',0),x_{t+1:L}|y_{t}=(j,0)\right)= & P\left(x_{t+2:L}|y_{t+1}=(j',0),y_{t}=(j,0),x_{t+1}\right)\\
 & \cdot P\left(x_{t+1}|y_{t+1}=(j',0),y_{t}=(j,0)\right)\\
 & \cdot P\left(y_{t+1}=(j',0)|y_{t}=(j,0)\right)
\end{align*}

\end_inset


\end_layout

\begin_layout Standard
Using the conditional independencies to simplify the probabilities:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{align*}
P\left(y_{t+1}=(j',0),x_{t+1:L}|y_{t}=(j,0)\right)= & P\left(x_{t+2:L}|y_{t+1}=(j',0)\right)\\
 & \cdot P\left(x_{t+1}|y_{t+1}=(j',0),y_{t}=(j,0)\right)\\
 & \cdot P\left(y_{t+1}=(j',0)|y_{t}=(j,0)\right)=\\
= & \beta_{j',t+1}\cdot E_{u,x_{t-o+2},...,x_{t+1}}\cdot T_{j,j'}
\end{align*}

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Formula $P\left(y_{t-|W_{l}|:t-1}=(j',l),y_{t-|W_{l}|-1}=(j,0),x_{1:t}\right)=\,P\left(y_{t-|W_{l}|-1}=(j,0),x_{1:t-|W_{l}|-1}\right)\cdot\cdot P\left(y_{t-|W_{l}|:t-1}=(j,l)|y_{t-|W_{l}|-1}=(j,0),x_{1:t-|W_{l}|-1}\right)\cdot P\left(x_{t-|W_{l}|:t-1}|y_{t-|W_{l}|:t-1}=(j,l),y_{t-|W_{l}|-1}=(j,0),x_{1:t-|W_{l}|-1}\right)\cdot P\left(x_{t}|y_{t}=(j,0),x_{1:t-1},y_{t-|W_{l}|:t-1}=(j,l),y_{t-|W_{l}|-1}=(j,0)\right)$
\end_inset


\end_layout

\begin_layout Standard
For the sub-state transition term, using the chain rule:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{align*}
P\left(y_{t+1:t+\left|W_{l}\right|}=(j,l),y_{t+\left|W_{l}\right|+1}=(j,0),x_{t+1:L}|y_{t}=(j,0)\right)= & P\left(x_{t+\left|W_{l}\right|+2:L}|x_{t+1:t+\left|W_{l}\right|+1},y_{t+1:t+\left|W_{l}\right|}=(j,l),y_{t+\left|W_{l}\right|+1}=(j,0),y_{t}=(j,0)\right)\\
 & \cdot P\left(x_{t+\left|W_{l}\right|+1}|x_{t+1:t+\left|W_{l}\right|},y_{t+1:t+\left|W_{l}\right|}=(j,l),y_{t+\left|W_{l}\right|+1}=(j,0),y_{t}=(j,0)\right)\\
 & \cdot P\left(x_{t+1:t+\left|W_{l}\right|}|y_{t+1:t+\left|W_{l}\right|}=(j,l),y_{t+\left|W_{l}\right|+1}=(j,0),y_{t}=(j,0)\right)\\
 & \cdot P\left(y_{t+1:t+\left|W_{l}\right|}=(j,l),y_{t+\left|W_{l}\right|+1}=(j,0)|y_{t}=(j,0)\right)
\end{align*}

\end_inset


\end_layout

\begin_layout Standard
Using the conditional independencies to simplify the probabilities:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{align*}
P\left(y_{t+1:t+\left|W_{l}\right|}=(j,l),y_{t+\left|W_{l}\right|+1}=(j,0),x_{t+1:L}|y_{t}=(j,0)\right)= & P\left(x_{t+\left|W_{l}\right|+2:L}|y_{t+\left|W_{l}\right|+1}=(j,0)\right)\\
 & \cdot P\left(x_{t+\left|W_{l}\right|+1}|y_{t+\left|W_{l}\right|+1}=(j,0)\right)\\
 & \cdot P\left(x_{t+1:t+\left|W_{l}\right|}|y_{t+1:t+\left|W_{l}\right|}=(j,l)\right)\\
 & \cdot P\left(y_{t+1:t+\left|W_{l}\right|}=(j,l),y_{t+\left|W_{l}\right|+1}=(j,0)|y_{t}=(j,0)\right)=\\
= & \beta_{j,t+|W_{l}|+1}\cdot E_{j,x_{t-o+|W_{l}|+2},...,x_{t+|W_{l}|+1}}\cdot L_{W_{l}}\left(x_{t+1},...,x_{t+|W_{l}|}\right)\cdot G_{j,l}
\end{align*}

\end_inset


\end_layout

\begin_layout Standard

\series bold
\bar under
M-Step
\end_layout

\begin_layout Standard
First we calculate auxiliary variables:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{align*}
\psi_{i,j,l,t}= & P\left(y_{t}^{i}=(j,0),y_{t+1:t+|W_{l}|}^{i}=(j,l),y_{t+|W_{l}|+1}^{i}=(j,0),X_{i}\right)\\
= & \alpha_{i,j,t}\cdot F_{j}\cdot G_{j,l}\cdot L_{W_{l,}}\left(x_{t+1}^{i},...,x_{t+|W_{l}|}^{i}\right)\cdot E_{j,x_{t+|W_{l}|-o+2}^{i},...,x_{t+|W_{l}|+1}^{i}}\cdot\beta_{i,j,t+|W_{l}|+1}
\end{align*}

\end_inset


\begin_inset Newline newline
\end_inset


\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{align*}
\gamma_{i,j,t}= & P\left(y_{t}^{i}=(j,0)|X_{i}\right)=\frac{P\left(y_{t}^{i}=(j,0),X_{i}\right)}{P\left(X_{i}\right)}\\
= & \frac{\alpha_{i,j,t}\cdot\beta_{i,j,t}}{\underset{j'\in\{1,...,m\}}{\sum}\left(\alpha_{i,j',t}\cdot\beta_{i,j',t}+\underset{l\in\{1,...,k\}}{\sum}\underset{s\in\{1,...,|W_{l}|\}}{\sum}\psi_{i,j',l,t-s}\right)}
\end{align*}

\end_inset


\begin_inset Newline newline
\end_inset

 
\begin_inset Newline newline
\end_inset

 
\begin_inset Newline newline
\end_inset

 
\begin_inset Formula 
\[
P\left(y_{t}=(j,0),x_{1:L}\right)=P\left(y_{t}=(j,0),x_{1:t}\right)\cdot P\left(x_{t+1:L}|x_{1:t},y_{t}=(j,0)\right)\approx P\left(y_{t}=(j,0),x_{1:t}\right)\cdot P\left(x_{t+1:L}|y_{t}=(j,0)\right)=\alpha_{j.t}\cdot\beta_{j,t}
\]

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
\approx=
\]

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Formula $P\left(x_{1:L}\right)=\underset{j\in\{1,...,m\}}{\sum}\alpha_{j,1}\cdot\beta_{j,1}=\underset{j\in\{1,...,m\}}{\sum}P\left(y_{1}=(j,0)|x_{1}\right)\cdot P\left(x_{2:L}|y_{1}=(j,0)\right)=\underset{j\in\{1,...,m\}}{\sum}\frac{P\left(y_{1}=(j,0),x_{1}\right)}{P\left(x_{1}\right)}\cdot\frac{P\left(y_{1}=(j,0),x_{2:L}\right)}{P\left(y_{1}=(j,0)\right)}=$
\end_inset


\begin_inset Newline newline
\end_inset

 
\begin_inset Newline newline
\end_inset


\end_layout

\begin_layout Standard
TODO: does different t gives different 
\begin_inset Formula $P\left(x_{1:L}\right)=\underset{j'\in\{1,...,m\}}{\sum}\alpha_{i,j',t}\cdot\beta_{i,j',t}$
\end_inset

? Should it?
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{align*}
\xi_{i,j_{1},j_{2},t}= & P\left(y_{t}^{i}=(j_{1},0),y_{t+1}^{i}=(j_{2},0)|X_{i}\right)=\frac{P\left(y_{t}^{i}=(j_{1},0),y_{t+1}^{i}=(j_{2},0),X\right)}{P\left(X_{i}\right)}\\
\\
= & \frac{\alpha_{i,j_{1},t}\cdot T_{j_{1},j_{2}}\cdot E_{j_{2},x_{t-o+2}^{i},...,x_{t+1}^{i}}\cdot\beta_{i,j_{2},t+1}}{\underset{j'_{1},j'_{2}\in\{1,...,N\}}{\sum}\alpha_{i,j'_{1},t}\cdot\left(1-F_{j'_{1}}\right)\cdot T_{j'_{1},j'_{2}}\cdot E_{j'_{2},x_{t-o+2}^{i},...,x_{t+1}^{i}}\cdot\beta_{i,j'_{2},t+1}+\underset{j',\in\{1,...,N\}\,l\in\{1,...,k\}}{\sum}\left(\underset{s\in\{0,...,|W_{l}|\}}{\sum}\psi_{i,j',l,t-s}\right)}
\end{align*}

\end_inset


\begin_inset Newline newline
\end_inset

 TODO: maybe denote a new variable to make above formula nicer?
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{align*}
\eta_{i,j,l,t}= & P\left(y_{t}^{i}=(j,0),y_{t+1:t+|W_{l}|}^{i}=(j,l)|X_{i}\right)\\
= & \frac{P\left(y_{t}^{i}=(j,0),y_{t+1:t+|W_{l}|}^{i}=(j,l),X_{i}\right)}{P\left(X_{i}\right)}\\
= & \frac{\psi_{i,j,l,t}}{\underset{j'\in\{1,...,m\}}{\sum}\left(\alpha_{i,j',t}\cdot\beta_{i,j',t}+\underset{l\in\{1,...,k\}}{\sum}\underset{s\in\{1,...,|W_{l}|\}}{\sum}\psi_{i,j',l,t-s}\right)}
\end{align*}

\end_inset


\end_layout

\begin_layout Standard
We use the temporary auxiliary variables to calculate the 
\begin_inset Formula $\theta_{max}$
\end_inset

 that maximizes likelihood of the observations.
\end_layout

\begin_layout Standard
TODO: add mid steps to the calculation to make more readable
\end_layout

\begin_layout Standard
\begin_inset Formula $E_{j,b_{1},b_{2},...,b_{o}}=\frac{\underset{i\in[N]\,t\in[L]}{\sum}\gamma_{i,j,t}\cdot\boldsymbol{1}_{b_{1},...,b_{o}}(x_{t-o+1}^{i},...,x_{t}^{i})}{\underset{i\in[N]\,t\in[L]}{\sum}\gamma_{i,j,t}}$
\end_inset


\end_layout

\begin_layout Standard
\begin_inset Formula $T_{j_{1},j_{2}}=\frac{\underset{i\in[N]\,t\in[L]}{\sum}\xi_{i,j_{1},j_{2},t}}{\underset{i\in[N]\,t\in[L]}{\sum}\gamma_{i,j_{1},t}}$
\end_inset


\end_layout

\begin_layout Standard
\begin_inset Formula $F_{j}=\frac{\underset{i\in[N]\,t\in[L]\,l\in[k]}{\sum}\eta_{i,j,l,t}}{\underset{i\in[N]\,t\in[L]}{\sum}\gamma_{i,j,t}}$
\end_inset


\end_layout

\begin_layout Standard
\begin_inset Formula $G_{j,l}=\frac{\underset{i\in[N]\,t\in[L]}{\sum}\eta_{i,j,l,t}}{\underset{i\in[N]\,t\in[L],\,l'\in[k]}{\sum}\eta_{i,j,l',t}}$
\end_inset


\end_layout

\begin_layout Standard
\begin_inset Formula $\pi_{j}=\frac{\gamma_{i,j,1}}{\underset{i\in[N]\,j'\in[m]}{\sum}\gamma_{i,j',1}}$
\end_inset


\begin_inset Newline newline
\end_inset

 
\begin_inset Newline newline
\end_inset

 
\begin_inset Newline newline
\end_inset

 
\begin_inset Newline newline
\end_inset

 [Roadmap enhancers preprocessing]
\end_layout

\begin_layout Standard
[training on roadmap data]
\end_layout

\begin_layout Standard
[classification of regulation modules]
\end_layout

\begin_layout Section*
Results
\end_layout

\begin_layout Standard
[test accuracy on roadmap enhancers]
\end_layout

\begin_layout Standard
[prediction on roadmap regulation modules]
\end_layout

\begin_layout Standard
[Whole genome classification?]
\end_layout

\begin_layout Standard
[Was HOP-HMM better?]
\begin_inset Newline newline
\end_inset


\end_layout

\begin_layout Standard

\series bold
\bar under
Possible Applications
\end_layout

\begin_layout Standard
labeled enhancer seqs from multiple motifs-> EM to learn E M F per floor
 + setting 
\begin_inset Formula $T=\mathbb{I}_{m\times m}$
\end_inset

 -> posterior of whole genome with sliding window -> classify whole genome
\end_layout

\begin_layout Standard
learn E M F -> check correlation with TF expression
\end_layout

\begin_layout Standard
run EM on whole genome -> posterior of whole genome -> check correlation
 of posterior to ChIP-Seq of histone modifications
\end_layout

\begin_layout Standard
E M F T-> posterior of whole genome -> see if known critical SNPs are critical
 in classification
\end_layout

\begin_layout Section*
Discussion
\end_layout

\begin_layout Section*
References
\end_layout

\end_body
\end_document
